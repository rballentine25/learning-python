{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Supervised vs unsupervised__\n",
    "__Supervised learning:__\n",
    "* input data is paired with corresponding output labels\n",
    "* aims to find a relationship between input variables and desired output\n",
    "* given input-output pair training sets, where the goal is to minimize difference between algorithm's predicted output and the actual output\n",
    "* divided into two types: regression and classification\n",
    "* supervised learning is tricky since overfitting can happen more easily. have to look more closely at # of dimensions vs # of data points\n",
    "\n",
    "__Unsupervised learning:__\n",
    "* algorithm is given input data without instructions on what to do with it. The goal is to find patterns/structures/relationships within the data without labeled output\n",
    "* exploration of structure within the data: finding clusters of similar points, finding outliers, reducing dimenionsality, discovering patterns, etc\n",
    "* several types: clustering (k-means and hierarchical), dimionsionality reduction (PCA and t-SNE), association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Unsupervised Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: K-means and Hierachical\n",
    "clustering algorithms try to group data points by organizing based on patterns/variations. Clusters are assigned so that all points in a group are more similar to each other than the points in other groups. \n",
    "\n",
    "### K-Means\n",
    "* K-means groups data points into clusters by assigning data points to one of a __pre-specified number of K clusters__ depending on the point's distance to the center of each cluster\n",
    "* data points that are closer together are considered more similar, and data points farther apart are considered less similar\n",
    "\n",
    "* K is the number of clusters, which is an input variable\n",
    "* the original cluster centroids are randomly selected\n",
    "* each data point is iteratively assigned to the nearest centroid by minimizing the euclidean distance from the point to the centroid of each cluster\n",
    "* after all points are assigned to a cluster based on the original centroids, new centroids are chosen and the process is repeated.\n",
    "* this continues until the centroids converge\n",
    "\n",
    "steps:\n",
    "1. K number of cluster centroids are randomly (?) selected (clusters have no assigned points to start)\n",
    "2. each data point is assigned to a cluster based on the distance between the point and the centroids of each cluster. the point is assigned to the cluster with the minimum distance \n",
    "3. after ALL data points are assigned to a cluster, the cluster centroids are recalculated based on their assigned points\n",
    "4. all data points are \"removed\" from their clusters and step 2 is repeated based on the new centroid locations\n",
    "5. steps 2-4 are repeated until the clusters no longer change significantly or there is no change in cluster assignment for the points (convergence of the cluster centroids)\n",
    "\n",
    "visualization: \"K-Means Clustering Explanation and Visualization\", TheDataPost (https://www.youtube.com/watch?v=R2e3Ls9H_fc)\n",
    "\n",
    "### Hierarchical Cluster Analysis (HCA)\n",
    "hierarchical clustering creates groups that are visually represented in a dendrogram (tree line graph). the tree shows the relationships between points and shows how they are clustered on different levels. Two steps are repeated: 1. identify the 2 clusters that are closest together 2. merge the 2 maximum comparable clusters\n",
    "\n",
    "two kinds of hierarchical clusters: __Agglomerative__ and __Divisive__\n",
    "* __Agglomerative__ (bottom up): each point is considered to be its own cluster. Clusters are then merged until a single cluster remains\n",
    "* __Divisive__: the opposite of agglomerative. all points are initially considered to be in one cluster, then points that are not similar are divided from the cluster until each point is its own cluster.\n",
    "\n",
    "steps in HCA:\n",
    "1. calculate similarity of one cluster with all other clusters, which creates a proximity/distance matrix\n",
    "2. merge the clusters which are highly similar or close to each other\n",
    "3. recalculate the distance matrix for each cluster\n",
    "\n",
    "first step is calculating a nxn distance matrix for distance btween each pair of objects (Euclidean distance is usually used). \n",
    "next step is finding similarity between clusters, which can depend on noise in the data set, the shape of the clusters, and the density of data points. \n",
    "\n",
    "\n",
    "__methods for finding similarity:__\n",
    "* minimum single linkage: distance between clusters is the distance between the point in the first cluster that is nearest to a point in the other cluster. won't work as well if there is noise between clusters\n",
    "* maximum complete linkage: distance between clusters is the maximum distance between points in two clusters. this is the distance between the points in each cluster that are the farthest away from each other. can tend to break large clusters\n",
    "* centroid linkage: distance between clusters is the distance between their centroids\n",
    "* average linkage: distance between clusters is the averaged distance between all pairs of points in the clusters\n",
    "\n",
    "space and time complexity: space is O(n^2) for an nxn proximity matrix. time is O(n^3) for n iterations, where each calculates the nxn matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Principal Components Analysis (PCA)\n",
    "### PCA\n",
    "* dimensionality reduction method to reduce dimensionality of large data sets in order to more easily understand them\n",
    "* exchanging accuracy for simplicity; goal of PCA is to reduce the number of variables while preserving as much info as possible\n",
    "* principal components are new variables made of linear combos of initial variables. Each dimension is compressed to one principal component (ex. 5 dimenional data = 5 principal components) \n",
    "* maximum info is in 1st component, max remaining in 2nd component, etc\n",
    "* principal components represent the directions of the data with the __maximum amount of variance__\n",
    "\n",
    "how principal components are constructed:\n",
    "* 1st component finds the __largest possible variance__ in the data set (where the points are most spread out)\n",
    "* 2nd component must be __orthogonal to 1st component__, with the max possible variance with that condition in mind\n",
    "* 3rd component is orthogonal to 2nd comp with max variance, and so on\n",
    "\n",
    "steps:\n",
    "1. standardization of all variables\n",
    "2. covariance matrix computation, ex. __[__ cov(x,x) cov(x,y) ; cov(y,x) cov(y,y )__]\n",
    "3. find eigenvectors and eigenvalues of covariance matrix: the eigenvectors are actually the principal components, and the eigenvalues give the total amount of variance for each principal component!\n",
    "\n",
    "additional references:\n",
    "https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Supervised Learning: LDA__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "* LDA is similar to PCA, but instead of maximizing variance, it finds largest clusters of classes and then finds the center of each class. \n",
    "* wants to know how far away from the centroid each member of a class is \n",
    "* the distances between the centroids can be made into ratios: Sw is within class scatter (small), Sb is between class scatter (big). Sb/Sw -> Sw^(-1)Sb makes a dxd matrix, and then eigenvectors can be found for components. \n",
    "\n",
    "\n",
    "* when creating a new axis, LDA aims to project data in a way that maximizes the separation of data points within difference classes. the goal is to maximize the distance between the centers of the two classes while also minimizing the variation within each class (class clusters, with each class cluster far apart from the others)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
