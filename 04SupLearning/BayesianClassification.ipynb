{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification\n",
    "### conditional probablility\n",
    "probability of an event occurring given that another event has already occurred. A conditional probability is P(X|Y), probability of X given Y\n",
    "\n",
    "### bayes theorem\n",
    "given a feature vector X = (x1, x2, x3,...)\n",
    "need to find appropriate label\n",
    "\n",
    "then given probabality P(Y = y | X = (x1, x2, x3...))\n",
    "need to find this conditional probablity for all possible values of Y\n",
    "for what value of y is the probablity maximum?\n",
    "but P(Y|X) is hard to find. \n",
    "\n",
    "Bayes theorem says that:\n",
    "P(C|X) = [ P(X|C)*P(C) ] / P(X)\n",
    "P(C|X) is posterior, P(C) is prior, P(X) is evidence, P(X|C) is likelihood\n",
    "ie what is the probability of C, given that X is true, where C is the class/label and X is the feature set\n",
    "<br> probability of A given B = probability of B given A times probability of A, divided by probabillity of B\n",
    "\n",
    "\n",
    "### maximum likelihood classifier\n",
    "Fit a distribution to the data\n",
    "ML classifier assigns class to a data point based on which class makes the observed data most probable. The idea is to find the class C that maximizes the likelihood (cond prob) function P(X|C) where X is the observed data/features. \n",
    "\n",
    "## what about multiple features?\n",
    "### naive bayes classifier\n",
    "naive bayes assumes that every pair of features is *independant* of each other, and that all features have the same weight. \n",
    "\n",
    "then the joint likelihood P(X|C) can be assumed as the product of individual likelihoods: <br>\n",
    "P(X|C) = product(P(Xi|C)) = P(x1|C) * P(x2 |C) * ... *P(xn |C)\n",
    "\n",
    "posterier probability then bbecomnes:<br>\n",
    "P(C|X) = [ P(X|C)*P(C)] / P(X)\n",
    "\n",
    "and the assigned class is the one where the posterier probability is maximum.\n",
    "\n",
    "## steps for classifying a data point uaing naive bayes\n",
    "### with three features and two classes\n",
    "1. estimate likelihoods: P(x1|c1), P(x2|c1), P(x3|c1), P(x1|c2)...\n",
    "2. estimate priors: P(c1), P(c2)\n",
    "3. compute posterior: <br>\n",
    "P(c1|x) proportional to P(x1|c1) * P(x2|c1) * P(x3|c1) <br>\n",
    "P(c2|x) proportional to P(x1|c2) * P(x2|c2) * P(x3|c2)<br>\n",
    "4. classify based on which is higher, P(c1|X) or P(c2|X)\n",
    "\n",
    "### summary:\n",
    "naive bayes reduces the amount of data and required computation by assuming that all feature pairs are independant of each other -> posterior probability (probability of class c given feature x) is calculated with a product of all P(Xi|C) (probability of feature x given class C)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
