{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "* binary tree with two kinds of nodes: decision nodes and leaf nodes\n",
    "* at root node, the data is split based on the first decision node (at the root)\n",
    "* a node is a leaf node if it only contains points from a single class\n",
    "* the binary tree continues splitting into decision nodes and leaf nodes until all nodes are leaf nodes containing only single class points\n",
    "* decision nodes can be boolean, with yes and no being right and left nodes\n",
    "* once the tree is completed, the tree should be able to classify a new node into a class based on the class of the leaf node it ends up in\n",
    "* decision tree operates like a bunch of nested ifelse statements\n",
    "\n",
    "\n",
    "## finding splitting conditions: information theory\n",
    "* the splitting conditions for decision nodes are where machine learning comes in. <br>\n",
    "* information theory: the model should choose the spliting condition that maximizes information gain\n",
    "* at the root node, the uncertainty is the highest, so the least amount of info is known\n",
    "* __entropy__ is used to quantify the uncertainty, where entropy is the measure of information contained in a state or the measure of uncertainty/disorder in the distribution of class labels at a particular node. entropy gives us the level of purity of the data at a node.  \n",
    "* __information gain__ measures the reduction in entropy that results from splitting the data. high IG means good splitting condiion, low is bad\n",
    "\n",
    "### entropy & info gain\n",
    "$$ entropy = \\sum_{}^{} pi * log(pi), E <= 1$$\n",
    "where pi is the probability of class i. \n",
    "<br>\n",
    "to find information gained:\n",
    "$$ IG = E(parent) - wi(child) $$\n",
    "where the second thing is the weighted average entropy of the child. \n",
    "weighted average entropy is given by: \n",
    "$$ wi = \\sum_{i=1}^{k} \\frac{|Ci|}{n} * \\sum_{j=1}^{k} Pji * log(Pji)$$\n",
    "where Ci is the ith cluster and Pji = P(Dj/Ci) where Dj is actual jth class. k is number of classes\n",
    "\n",
    "<br> model will calculate IG comparision for every possible split and choose the one that maximized IG (by minimizing entropy)\n",
    "<br> two approaches:\n",
    "* greedy approach will select the attribute that provides max info gain at every step\n",
    "* random will randomly select attributes and evaluate their performance, which helps prevent overfitting\n",
    "\n",
    "### pruning\n",
    "* a tree with a large number of splits is prone to overfitting and improve generalization\n",
    "* this can be helped by __pruning__ which is removing branches with little significance. can be done while growing the tree (pre-rpuning) or after the tree is built to depth (post-pruning)\n",
    "\n",
    "pre-pruning: introducing stopping criteria\n",
    "* maximum depth (avoids overfitting)\n",
    "* minimum samples/leaf (dont select split if resulting leaf is below minimum)\n",
    "* minimum info gain (stop when IG falls below minimum)\n",
    "\n",
    "post-pruning\n",
    "* cost-complexity pruning (CCP): assigns a price to each subtree based on accuracy and complexity, then selects the subtree with the lowest \"fee\"\n",
    "* reduced error pruning: removes branches that dont significantly affect accuracy by comparing new error to a validation set\n",
    "* minumum impurity decrease\n",
    "* minimum leaf size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble classifiers\n",
    "Ensemble classifiers combine predictions of multiple weak classifiers to create a strong classifier. It reduces misclassification rate and improves generalizability by aggregating several weak classifiers.\n",
    "\n",
    "### simple ensemble techniques:\n",
    "1. max voting: generally used for classification problems. Multiple models are used to make predictions for a data point, and the result with the max vote is the final prediction. \n",
    "2. averaging: average of all predictions is calculated and result is final prediction. useful for regression problems or when calculating probabilities for classification problems\n",
    "3. weighted average: all models are assigned different weights for importance in prediction. then the average is calculated (similar to 2), including these weights\n",
    "\n",
    "### advanced ensemble techniques\n",
    "bagging vs boosting: bagging models train in parallel; boosting models train sequentially. typically, bagging is used for weak learners with high variance/low bias, and boosting is the opposite.  \n",
    "* __bagging__: basic idea is combining results of multiple models to get a generalized result. However, this is not the most useful when the models are training on the same set of data (they all have the same input). \n",
    "<br> first, a number of subsets are created from the original set using bootstrapping. these subsets may be smaller than the original set. A base model is created using each of these subsets as training data, and the final prediction model is the aggragate of all of these independant models.  \n",
    "<br> *bootstrapping* is a sampling technique that creates subsets from the original dataset, with replacement (items drawn from original dataset will not be removed and can be drawn again). Each sample then represents a randomly chosen subset of the entire population. \n",
    "<br> *Out Of Bag (OOB) score*: after each bootstrap sample is selected, the points that were not chosen from the original set are given back to the model trained on that set as \"unseen data\" (test data). For every point, the trees whose sample did not include that point are used to predict the class of that data point. the final prediction will be used by max voting of these trees. the final OOB score will be found by aggregating all OOB predictions and comparing them to the true labels. \n",
    "<br> NOTE: due to the nature of sampling with replacement, when all samples have been bagged, only 63.2% of the orignal samples have been drawn. that leaves about 36.8% to be used as OOB data.\n",
    "<br><br>\n",
    "* __boosting__: boosting is sequential, where each subsequent model attempts to correct the errors of the previous model. \n",
    "<br> a subset is created from the original and all points are given equal weights. a base model is created using this subset, then makes predictions on the WHOLE dataset. errors are calculated using actual/predicted values and the incorrectly predicted points are given higher weights (this allows the algorithm to identify the parameters to focus on to improve performance). a new model is made and is used to predict the data with the new weights. this is repeated until either all points are correctly identified or the max number of models have been created. \n",
    "<br> __QUESTION__: how does weighting make the model change? what does it mean to weight the incorrectly predicted points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest model: bagging\n",
    "RF model uses bagging with bootstrapping to generate sampled-with-replacement subsets for each decision tree. Because each DT is trained separately on different bootstrap samples before being combined into a ensemble model, the variations in the original dataset dont impact the final aggregate result. bagging then reduces variance without changing the bias of the complete ensemvle. \n",
    "<br>\n",
    "* RF algorithm has 3 main parameters: node size, number of trees, and number of features sampled (how many features can be looked through at each node).\n",
    "* random subsets created via boostrapping\n",
    "* of each bootstrap sample, one third is test data (OOB) and the rest is the training data.  \n",
    "* at each node, the algorithm is limited to a random selection of features from which to search for the highest info gain (ie resulting choice may not have highest overall info gain, but ensures that the resulting trees have less correlation)\n",
    "* DT model fitted to each bootstrap subset\n",
    "* final prediction calculated by averaging prediction of all decision trees\n",
    "\n",
    "<br> differences from regular trees: bootstrapping means multiple trees are made. feature sampling at each node means the split with the highest IG may not necessarily be chosen, meaning that the resulting trees have less likeliness of correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting \n",
    "* boosting is the concept of improving a single weak learner model by combining it with a number of other weak learner models in order to make a collectively strong learner model. \n",
    "<br> weak learners have low prediction accuracy, only a little better than random guessing. the combination of running several weak learners sequentially produces a stronger learner. \n",
    "<br> the main difference between bagging and boosting is that bagging is done in parallel on multiple datasets, while boosting is done sequentially on the same datset. \n",
    "\n",
    "### AdaBoost (adaptive boosting)\n",
    "* adaboost gives same weight to each dataset initially\n",
    "* auto adjusts weights of the points after every decision tree, giving more weight to incorrectly classes points. \n",
    "\n",
    "### Gradient boosting\n",
    "* differes from AdaBoost: doesn't adjust weights, instead optimizes loss functionby generating base learners sequentially so the present learner is always more effective than previous one. aims for more accurate results initially rahter than correcting errors\n",
    "\n",
    "### XG-boost \n",
    "* Extreme Gradient Boosting: improved version of GB for computational speed and scale\n",
    "* ability to handle data with missing values\n",
    "* built in support for parallel processing\n",
    "* implementation of gradient boosted decision trees\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
