{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "* binary tree with two kinds of nodes: decision nodes and leaf nodes\n",
    "* at root node, the data is split based on the first decision node (at the root)\n",
    "* a node is a leaf node if it only contains points from a single class\n",
    "* the binary tree continues splitting into decision nodes and leaf nodes until all nodes are leaf nodes containing only single class points\n",
    "* decision nodes can be boolean, with yes and no being right and left nodes\n",
    "* once the tree is completed, the tree should be able to classify a new node into a class based on the class of the leaf node it ends up in\n",
    "* decision tree operates like a bunch of nested ifelse statements\n",
    "\n",
    "\n",
    "## finding splitting conditions: information theory\n",
    "* the splitting conditions for decision nodes are where machine learning comes in. <br>\n",
    "* information theory: the model should choose the spliting condition that maximizes information gain\n",
    "* at the root node, the uncertainty is the highest, so the least amount of info is known\n",
    "* __entropy__ is used to quantify the uncertainty, where entropy is the measure of information contained in a state or the measure of uncertainty/disorder in the distribution of class labels at a particular node. entropy gives us the level of purity of the data at a node.  \n",
    "* __information gain__ measures the reduction in entropy that results from splitting the data. high IG means good splitting condiion, low is bad\n",
    "\n",
    "### entropy & info gain\n",
    "$$ entropy = \\sum_{}^{} pi * log(pi), E <= 1$$\n",
    "where pi is the probability of class i. \n",
    "<br>\n",
    "to find information gained:\n",
    "$$ IG = E(parent) - wi(child) $$\n",
    "where the second thing is the weighted average entropy of the child. \n",
    "weighted average entropy is given by: \n",
    "$$ wi = \\sum_{i=1}^{k} \\frac{|Ci|}{n} * \\sum_{j=1}^{k} Pji * log(Pji)$$\n",
    "where Ci is the ith cluster and Pji = P(Dj/Ci) where Dj is actual jth class. k is number of classes\n",
    "\n",
    "<br> model will calculate IG comparision for every possible split and choose the one that maximized IG (by minimizing entropy)\n",
    "<br> two approaches:\n",
    "* greedy approach will select the attribute that provides max info gain at every step\n",
    "* random will randomly select attributes and evaluate their performance, which helps prevent overfitting\n",
    "\n",
    "### pruning\n",
    "* a tree with a large number of splits is prone to overfitting and improve generalization\n",
    "* this can be helped by __pruning__ which is removing branches with little significance. can be done while growing the tree (pre-rpuning) or after the tree is built to depth (post-pruning)\n",
    "\n",
    "pre-pruning: introducing stopping criteria\n",
    "* maximum depth (avoids overfitting)\n",
    "* minimum samples/leaf (dont select split if resulting leaf is below minimum)\n",
    "* minimum info gain (stop when IG falls below minimum)\n",
    "\n",
    "post-pruning\n",
    "* cost-complexity pruning (CCP): assigns a price to each subtree based on accuracy and complexity, then selects the subtree with the lowest \"fee\"\n",
    "* reduced error pruning: removes branches that dont significantly affect accuracy by comparing new error to a validation set\n",
    "* minumum impurity decrease\n",
    "* minimum leaf size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
