{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural nets\n",
    "composed of node layers\n",
    "1. input layer\n",
    "2. hidden layers\n",
    "3. output layers\n",
    "\n",
    "uses linear regression model\n",
    "each node is composed of input, weight, bias/threshold, output\n",
    "\n",
    "ANNs: data is passed between nodes feed forward\n",
    "\n",
    "output is yhat = (w1*x1 + w2*x2 + ...) - threshold\n",
    "\n",
    "each feature is given a weight for importance\n",
    "\n",
    "NNs rely on training data and evaluate using a cost function (needs to be minimized)\n",
    "\n",
    "cost is minimized as the model adjusts weights and bias through gradient descent\n",
    "\n",
    "there are also Convolutional NNs (CNN) and Recurrent NNs (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multilayer perceptron\n",
    "the value inside a node/nueron is its \"activation\"\n",
    "output layer: activation is usually the confidence that the answer is the label with that node\n",
    "\n",
    "activations in one layer determine activations in the next layer\n",
    "\n",
    "the hope with layers is that tasks are broken down into supatterns during first few layers and reassembled layer in the the network\n",
    "\n",
    "each node is given a weight between it and a node in the next layer, and the weighted sum of all the activation inputs to that node is put through a regression function then assigned as the activation for that node. ie all nodes in the current layer will have the same number of weights/edges as the number of nodes in the next layer\n",
    "\n",
    "a bias is also added to the weighted sum before being put through the regression function. bias indicates the min value needed for the nueron to be considered active\n",
    "\n",
    "popular ones are sigmoid, rectified linar unit (ReLU), tanh\n",
    "\n",
    "when the algorithm is learning, it is tweaking all its \"knobs\" -> finding the most accurate weights and biases\n",
    "\n",
    "each nueron is like a function: it takes outputs from the previous layer as its inputs and spits out an activation output from those\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how do nueral nets learn?\n",
    "at the start, all weights are biases are assigned totally randomly\n",
    "\n",
    "cost functions are used to help train\n",
    "cost function: sum of squares of differences between given \"trash\" activations and the desired value for a certain input\n",
    "\n",
    "this cost is low when the answer is given with confidence, and high when there is not a clear answer. \n",
    "\n",
    "the average of costs for all training samples is added up and this is a method of determining how well the algoritm is doing\n",
    "\n",
    "basically, the cost function takes the weights/biases as its inputs and spits out a single number to quantify how good they are\n",
    "\n",
    "to find the input to minimize the function: for a more simple function, just take the derivative to find the maximum. however, when the function is more complex and may have local minima/maxima, its not quite as simple\n",
    "\n",
    "instead, a random point is chosen and a decision must be made to step a certain amount in a certain direction, based on slope -> trying to find local minimum\n",
    "\n",
    "### gradients\n",
    "gradient of a function gives the direction of steepest increase\n",
    "length of the gradient is indication for steepness of slope\n",
    "\n",
    "alg for minimizing function becomes:\n",
    "- compute del(C)\n",
    "- small step in -del(C) direction\n",
    "- repeat\n",
    "\n",
    "the gradient then is used to \"nudge\" the weights in a way that causes most rapid decrease to cost function\n",
    "\n",
    "to calculate this gradient: backpropogation\n",
    "\n",
    "in all: learning/training is done by minimizing a cost function. for this to work, the cost function has to be relatively smooth. this is also why ANs have continuous values rather than just on/off\n",
    "\n",
    "gradient descent:\n",
    "sign of gradient vector tells us whether the corresponding weight should be nudged up (pos) or down (neg)\n",
    "magnitudes tell you which weights matter the most to the cost function\n",
    "\n",
    "so the gradient vector tells us which weights would be best to change and by how much\n",
    "\n",
    "### layers\n",
    "multilayer perceptron doesnt actually pick up on patterns like talked about earlier (sub patterns that get reassembled). its pretty good, but its just making multiple choice guesses. multilayer perceptrons are \"old technology\"\n",
    "\n",
    "-> prompts the question, is the network actually learning or is it just memorizing the dataset\n",
    "\n",
    "### papers:\n",
    "\"a closer look at memoorization in deep networks\"\n",
    "\"the loss surfaces of multilayer networks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back propogation\n",
    "ways of changing the activation value of a node:\n",
    "- changing bias\n",
    "- changing wi in proportion to ai\n",
    "- changing ai in proportion to ai\n",
    "where i is the previous node\n",
    "\n",
    "but when considering how to adjust a weight/bias, its not just one output value that is being considered. there will be different nugdge values and signs for each weight/bias based on each output\n",
    "\n",
    "all of these proposed changes for each output are addded together, which is where the idea of backwards propogation comes from\n",
    "\n",
    "this entire back prop routine is done for EACH training example, where the desired nudge for each weight is recorded. these are averaged for each weight across all training examples.this averaged vector is proportional to the gradient descent vecor.\n",
    "\n",
    "### stochastic gradient descent\n",
    "HOWEVER: doing the sums for this for every single example and every single output would be extremely computationally expensive.\n",
    "INSTEAD: \n",
    "training data is shuffled and then divided into mini batches. then a gradient descent step si calculated with back prop over each mini batch and applied to the model. \n",
    "\n",
    "### backprop calculus\n",
    "not going to attempt to write here  \n",
    "[Backpropogation calculus by 3Blue1Brown on YT](https://www.youtube.com/watch?v=tIeHLnjs5U8)  \n",
    "[Backpropogation math example by Matt Mazur](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)  \n",
    "  \n",
    "basic premise: start with randomized weights and feed forward through network, calculating activations with activation function. next, calculate the total cost function for the output layer, then start working backwards through the layers to recalculate the weights based on this cost function output value. this is done using gradient descent vector. Once all weights have been recalculated from output layer back, feed back forward through the network to calculate new cost function. repeat until the changes are miniscule / cost function reaches a set threshold"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
