{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "residual nueral network (CNN)  \n",
    "as CNNs grow deeper in layers, vanishing gradients occur which negatively impacts performance -> vanishing gradients occurs when the gradient is back-propogated to many earlier layers, which means the gradient gets very small due to repeated multiplication.  \n",
    "ResNets include a skip connection to solve the vanishing gradient problem  \n",
    "\n",
    "Skip connections are applied before relu activation layer, and have to be the same size  \n",
    "\n",
    "Why do skip connections work?  \n",
    "- provide alternate shortcut path for gradient\n",
    "- allows model to learn and identify function, which means higer layer will perform at least as well as the lower layer  \n",
    "\n",
    "\n",
    "ResNet skip connections used in U-net (Res blocks)  \n",
    "\n",
    "ResNet operates in two stages:\n",
    "1. Creates multiple layers that are initially skipped (not used), which reuses activation functions from previous layers instead\n",
    "2. Second stage retrains the network and expands the \"residual\" layers that were skipped before. \n",
    "\n",
    "The residual block structure ADDS output from previous layers to the output of the current block, before applying the activation function. \n",
    "\n",
    "## pytorch resnet\n",
    "- can be done with 18-152 layers\n",
    "- can be pre-trained on ImageNet database or your own data  \n",
    "\n",
    "to use the pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models \n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# to use transfer learning (pretrain), set pretrained to True\n",
    "# if pretrained is False, weights are initialized randomly to start from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other options for number of layers: 34, 50, 101, 152  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sources\n",
    "- [\"What is ResNet? (with 3D Visualizations)\"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=video&cd=&cad=rja&uact=8&ved=2ahUKEwix-aapy5GMAxWySjABHXarHf0QtwJ6BAgOEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dnc7FzLiB_AY&usg=AOvVaw1Av1tAr_hVpKn2I1F2PfJG&opi=89978449), Prof. Ryan Ahmed, YT  \n",
    "- [\"Understanding and Coding a ResNet in Keras\"](https://medium.com/data-science/understanding-and-coding-a-resnet-in-keras-446d7ff84d33), Priya Dwivedi, Medium.com  \n",
    "- [\"PyTorch ResNet\"](https://www.run.ai/guides/deep-learning-for-computer-vision/pytorch-resnet), run.ai\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
