{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating classification accuracy\n",
    "## accuracy, precision, and recall\n",
    "__Accuracy__: how many data points were detected correctly. HOWEVER: if the dataset is not well balanced, or mistakes have varying impact, accuracy is not a good measure for performance. \n",
    "\n",
    "if a dataset consists of pictures of cats or dogs, and the objective is to find the dogs (cats are irrelevant), then two kinds of mistakes can be made: \n",
    "1. the ml model misses a dog (false negative)\n",
    "2. the model wrongly identifies a cat as a dog (false positive)\n",
    "where correctly identified dogs are true positives and correctly identified cats are true negatives. \n",
    "\n",
    "then instead of accuracy, these two concepts can be used to evaluate model relevancy:\n",
    "\n",
    "### precision\n",
    "found by dividing true positives by overall positives. also considered as the probability that any randomly selected item is a true positive.\n",
    "<br> precision quantifies the number of positive class predictions that actually belong to the positive class.\n",
    "$$ precision = \\frac{true positive (TP)}{true positive (TP) + false positive (FP)}$$\n",
    "\n",
    "<br> for all identified positives, how many are correctly positive\n",
    "\n",
    "### recall\n",
    "recall is the measure of relevant elements that were detected. it divided true positives by the number of relevant elements. \n",
    "i.e. # of predicted dogs / total known # of dogs in the dataset\n",
    "<br> for a singular item, recall gives the probability that a randomly selected relevant item from the dataset will be detected.\n",
    "<br> recall quantifies the number of positive class predicitions made out of all positive examples in the dataset.\n",
    "$$ recall = \\frac{true positive (TP)}{true positive (TP) + false negative (FN)}$$\n",
    "\n",
    "<br> for ALL positives, which ones were correctly identified\n",
    "\n",
    "### F1 measure\n",
    "F1 = 2 * ((precision*recall) / (precision + recall))\n",
    "result is between 0.0 and 1.0, where 1.0 is the best.\n",
    "<br> F1 gives a score that balances precision and recall in one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation \n",
    "* validation: determining whether numerical results (which quantify hypothetical relationships) are acceptable descriptions of data\n",
    "* cross validation is used to get a better idea of the effectiveness of a model\n",
    "* main point of cross validation is preventing overfitting\n",
    "* simplest validation form is train-test split, where around 70% of the data is set as training data, and the other 30% is saved as validation data after the model is done training.\n",
    "* Mean squared error (MSE) is calculated on the predicted test set vs actual test set. \n",
    "* however, with only one training set and test set, MSE can be vastly different depending on what section of the dataset is chosen for train vs test\n",
    "* this is why we use cross validation instead\n",
    "\n",
    "### k fold validation\n",
    "* cross validation divides training data into multiple folds/subsets, where one fold at a time is used as validation data (rest is training data)\n",
    "* usually multiple folds are created and multiple tests are run, each using a different fold as validation data so that each test is exposed to a different set of \"new\" data\n",
    "* results of each test are averaged to produce a better performance estimate\n",
    "\n",
    "### leave one out CV: finding MSE of dataset\n",
    "* dataset is split into test/train, but only ONE datapoint out of n points is held as the testing set (n-1 points used for training)\n",
    "* model is trained on the training set and MSE is calculated. \n",
    "* this process is repeated n times, where each time a different singular datapoint is held as the test set \n",
    "* total MSE is found as average of the n test runs\n",
    "* cons: very time consuming and can be computationally expensive\n",
    "\n",
    "### bootstrapping\n",
    "*bootstrapping* is a sampling technique that creates subsets from the original dataset, with replacement (items drawn from original dataset will not be removed and can be drawn again). Each sample then represents a randomly chosen subset of the entire population. \n",
    "1. draw a sample of size N from the original dataset, with replacement\n",
    "2. repeat S times, so there are S bootstrap samples\n",
    "3. estimate on S samples, so there are S estimates\n",
    "4. combine these estimates to get a better estimate (or model)\n",
    "\n",
    "*Out Of Bag (OOB) score*: after each bootstrap sample is selected, the points that were not chosen from the original set are given back to the model trained on that set as \"unseen data\" (test data). For every point, the trees whose sample did not include that point are used to predict the class of that data point. the final prediction will be used by max voting of these trees. the final OOB score will be found by aggregating all OOB predictions and comparing them to the true labels. \n",
    "<br> NOTE: due to the nature of sampling with replacement, when all samples have been bagged, only 63.2% of the orignal samples have been drawn. that leaves about 36.8% to be used as OOB data.\n",
    "\n",
    "### AUC \n",
    "\n",
    "# QUESTIONS\n",
    "* what is the difference between jackknife and LOO? everytime i look jackknife up, LOO is the only thing that comes up \n",
    "* what does the difference between precsision and recall actually mean \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
