{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN: Generative Adversarial Netowrk \n",
    "*Sources:*  \n",
    "`\"What is a GAN?\", AWS. https://aws.amazon.com/what-is/gan/`  \n",
    "`\"What are GANs (Generative Adversarial Networks)?, IBM Technology, YT.com. https://www.youtube.com/watch?v=TpMIssRdhco`  \n",
    "`Generative Adversarial Network (GAN), Geeksforgeeks.org. https://www.geeksforgeeks.org/generative-adversarial-network-gan/`   \n",
    "`DCGAN Tutorial, Nathan Inkawhich, PyTorch.org. https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html`  \n",
    "\n",
    "## Overview\n",
    "A GAN trains two neural networks to compete against eeach other in order to generate more authentic data from a given training dataset. A GAN can generate new images from an existing database, etc. The first network creates new data by modifying a random input vector as much as possible, then the second network determines whether the created data is original or generated. The GAN continues learning/improving the generated data until the second network can't tell the difference between the original set and the generated samples.   \n",
    "\n",
    "In machine learning, GAN models can be used for data augmentation by creating synthetic data. This can be used to create a better dataset, to fill in missing values, etc etc.  \n",
    "\n",
    "GANS are a type of NN used for Unsupervised learning. \n",
    "\n",
    "## Structure\n",
    "A GAN is made up of a __generative__ network and a __discriminator__ netowrk.  \n",
    "Basic steps:\n",
    "1. generator analyzes training set and identifies attributes in data\n",
    "2. distriminator also analyzes training set and distinguishes between attributes independantly\n",
    "3. generator modifies data by adding noise or random changes to certain attributes\n",
    "4. generator passes modified data to discriminator\n",
    "5. discriminator calculates probability that generated output is from original dataset\n",
    "6. discriminator gives generator guidance to reduce randomization in next cycle  \n",
    "\n",
    "The discriminator is trained first; it is given the initial dataset and then after it is able to recognize attributes of the training data, it can be fed samples that don't belong to the original set to test whether it actually can discriminate. \n",
    "\n",
    "### How it works:\n",
    "The generator and discriminator go back and forth. The generator sends a generated sample to the discriminator, and the discriminator predicts whether is is fake or real. There is always a winner; if the disc determines the sample is rightfully fake, the discriminator wins, and if it is wrong, the generator wins. The loser modifies its model, and the winner remains unchanged. This cycle continues until the generator gets good enough that the discriminator is consistently wrong.   \n",
    "\n",
    "The discriminator's ouput is the probability that a sample came from the training set. It should be HIGH when x comes from the training set and LOW when x came from the generator. It can be thought of as a binary classifier.  \n",
    "\n",
    "The generator takes in a latent space vector, z. The generator will map z to the dataset space, and its goal is to estimate the distribution that the training data comes from, so it can generate samples that match that distribution.  \n",
    "\n",
    "G and D play a \"minimax\" game, where D tries to maximize the probability that it correctly classifies samples, and G tries to minimize the probability that D will predict it's outputs are fake. \n",
    "\n",
    "### DCGAN: Deep Convolutional GAN\n",
    "In a DCGAN, both models are CNNs.   \n",
    "\n",
    "The discriminator is made up of convolution layers, batch normalization layers, and leaky relu activations (recall that batch normalization layers normalize the output of the previous layer by subtracting mean and dividing by std to improve convergence\n",
    "). The input is a 3d input image (3rd dim is number of channels), and the output is the scalar probability.  \n",
    "\n",
    "The generator is made of convolutional transpose layers, batch norm layers, and relu activations. Input is latent vector z, and output is a 3d image. The conv-transpose layers transform the latent vector into a volume matching the output image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
