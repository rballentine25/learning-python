{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation\n",
    "*sources:* `https://www.ibm.com/think/topics/image-segmentation`  \n",
    "\n",
    "\n",
    "image segmentation processes image pixel by pixel. It uses various techniques to classify individual pixels as belonging to a certain class or instance. Deep learning models use NNs for pattern recognition and image segmentation. The ouputs of these models are __segmentation masks__ which represent pixel-by-pixel boundaries/shapes of each class in the image.   \n",
    "\n",
    "There are three types of image segmentation tasks, which relate to two types of classes. The classes are \"things\" and \"stuff\":\n",
    "- \"things\" are classes with specific, individual shapes. They are clearly defined and can be clearly counted. Ex. cars or trees or a person\n",
    "- \"stuff\" are classes that are not clearly formed or shaped, like sky or grass or water. They are not usually clearly defined or easily countable, and individual objects of these classes are not usually distinguishable from each other (think blades of grass)\n",
    "  \n",
    "Types of image seg:\n",
    "1. semantic segmentation\n",
    "treats all pixels as \"stuff\"; doesn't differentiate between \"stuff\" and \"things\". Doesn't distinguish between individual instances of certain classes, just recognizes the presence of a certain class.\n",
    "\n",
    "2. instance segmentation\n",
    "instance is kind of the opposite of semantic; it tries to recognize each individual instance of a certain class as separate objects. It ignores \"stuff\" completely and focuses completely on \"things\".   \n",
    "Instance seg algorithms usually do two-stage or one-shot approaches, where two-stage models are usually region-based CNNs. The CNN is used to generate bounding boxes for each instance, then more refined classification is done within the previously generated bounding box. \n",
    "\n",
    "3. panoptic segmentation\n",
    "panoptic combines a little of both the previous types. Each pixel is given a semantic label and an \"instance ID\", and pixels with same label and ID belong to the same object. IDs for \"stuff\" pixels are ignored.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder: U-net\n",
    "*sources:*  \n",
    "`https://pyimagesearch.com/2023/11/06/image-segmentation-with-u-net-in-pytorch-the-grand-finale-of-the-autoencoder-series/`  \n",
    "`https://www.geeksforgeeks.org/u-net-architecture-explained/`  \n",
    "\n",
    "U-net has inspired GAN variations including the Pix2Pix generator, which is what authors of \"Next Gen Wildfire...\" used.   \n",
    "\n",
    "U-Net consists of a encoder for downsampling and decoder for upsampling, connected by skip connections. Skip connections bybass the compressed latent rep and feed detailed feature maps from encoder directly to decoder. This helps to recover details lost during downsampling.   \n",
    "\n",
    "This means that during upsampling, the decoder isn't relying solely on the latent space representation for reconstruction. It can also use feature maps from corresponding layer sizes from the encoder to reconstruct samples.   \n",
    "\n",
    "As the spacial dimensions are decreased during encoding, the number of channels (depth) is increased. During decoding,spatial dimensions are increased by upsampling layers and reducing the amount of channels. The skip connections allow feature paths from encoder layers to be used by the deconder to reconstruct and identify features. Each pixel in the final output image has a label for a object/class int he input image: i.e. the output map is a binary segmentation map where each pixel represents either fire or no fire. \n",
    "\n",
    "### why would an autoencoder be used for image seg?\n",
    "A CAE can learn intrinsic features of images without explicit labels. The latent space representation is able to encapsulate essential structures, which can be used to cluster pixels according to class. They can also be trained as denoising autoencoders.   \n",
    "\n",
    "\n",
    "## convolutional autoencoder for image segmentation:\n",
    "given a multi channel input image, the encoder section uses convolutional layers to create feature maps. Pooling layers are used to decrease spatial dimensionality while increasing channel depth. The smallest spatial layer is the code/bottleneck layer, which should be the minimized, most essential feature representation of the original image. The decoder begins after the bottleneck, and uses deconvolutional/transpose-convolutional layers to upsample the spatial dimensions (and reduce the channel depth). The last layer in the decoder is a fc layer with only one output channel, which probably uses a sigmoid function or spomething similar to convert output into binary values. A threshhold is applied to the binary values to determine whether the output should be a 1 or a 0, and this output is what makes up the segmentation map.  \n",
    "\n",
    "now how they did prediction with that? idk\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
