{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoders (CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: https://www.geeksforgeeks.org/implement-convolutional-autoencoder-in-pytorch-with-cuda/\n",
    "  \n",
    "convolutional autoencoders replace regular feedforward NNs with CNNs for both encoder and decoder steps.   \n",
    "\n",
    "They are still made up of an encoder and a decoder, where the encoder processes the input image with decreasing size convolutional and pooling layers to decrease the dimensionality of the image. The decoder upsamples the lowest dimension representation using deconvolutional layers.   \n",
    "\n",
    "The network is trained to minimize difference between original input image and reconstructed using a loss function. THe encoder portion does feature extraction, and the decoder does image generation.   \n",
    "\n",
    "DECODER: done iwth convolutional and pooling layers to create lower dimensional feature representation.   \n",
    "ENCODER: takes the lower dimensional rep and upsamples using deconvolutional layers.    \n",
    "\n",
    "network is trained  to minimize difference between original input image and reconstructed output image using a loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder: U-net\n",
    "*sources:*  \n",
    "`https://pyimagesearch.com/2023/11/06/image-segmentation-with-u-net-in-pytorch-the-grand-finale-of-the-autoencoder-series/`  \n",
    "`https://www.geeksforgeeks.org/u-net-architecture-explained/`  \n",
    "\n",
    "U-net has inspired GAN variations including the Pix2Pix generator, which is what authors of \"Next Gen Wildfire...\" used.   \n",
    "\n",
    "U-Net consists of a encoder for downsampling and decoder for upsampling, connected by skip connections. Skip connections bybass the compressed latent rep and feed detailed feature maps from encoder directly to decoder. This helps to recover details lost during downsampling.   \n",
    "\n",
    "This means that during upsampling, the decoder isn't relying solely on the latent space representation for reconstruction. It can also use feature maps from corresponding layer sizes from the encoder to reconstruct samples.   \n",
    "\n",
    "As the spacial dimensions are decreased during encoding, the number of channels (depth) is increased. During decoding,spatial dimensions are increased by upsampling layers and reducing the amount of channels. The skip connections allow feature paths from encoder layers to be used by the deconder to reconstruct and identify features. Each pixel in the final output image has a label for a object/class int he input image: i.e. the output map is a binary segmentation map where each pixel represents either fire or no fire. \n",
    "\n",
    "### why would an autoencoder be used for image seg?\n",
    "A CAE can learn intrinsic features of images without explicit labels. The latent space representation is able to encapsulate essential structures, which can be used to cluster pixels according to class. They can also be trained as denoising autoencoders.   \n",
    "\n",
    "\n",
    "## convolutional autoencoder for image segmentation:\n",
    "given a multi channel input image, the encoder section uses convolutional layers to create feature maps. Pooling layers are used to decrease spatial dimensionality while increasing channel depth. The smallest spatial layer is the code/bottleneck layer, which should be the minimized, most essential feature representation of the original image. The decoder begins after the bottleneck, and uses deconvolutional/transpose-convolutional layers to upsample the spatial dimensions (and reduce the channel depth). The last layer in the decoder is a fc layer with only one output channel, which probably uses a sigmoid function or spomething similar to convert output into binary values. A threshhold is applied to the binary values to determine whether the output should be a 1 or a 0, and this output is what makes up the segmentation map.  \n",
    "\n",
    "now how they did prediction with that? idk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
