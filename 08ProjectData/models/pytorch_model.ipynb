{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F # for bce loss function\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Callable, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "     \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep\n",
    "## data visualization methods\n",
    "# do we need these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation metrics (DONE)\n",
    "- IoU: Intersection over Union. measures overlap between prediction region and actual (ground truth) region. Scores from 0 to 1, where 1 is perfect overlap and 0 is no overlap. Usually used to determine whether bounding box is correct\n",
    "- Recall\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IoU metric\n",
    "Calculation of intersection over union metric.\n",
    "    \n",
    "Args:\n",
    "    real_mask (Tensor): Ground-truth mask\n",
    "    predicted_mask (Tensor): Mask predicted by model\n",
    "Returns:\n",
    "    (float): IoU metric value\n",
    "\n",
    "CHANGES: \n",
    "    - changed \"tf\" to \"torch\" in method header and real_mask line\n",
    "    - added comments\n",
    "\"\"\"\n",
    "def IoU_metric(real_mask: torch.Tensor, predicted_mask: torch.Tensor) -> float: \n",
    "    # replacing neg values: torch.where(condition, choose-True, choose-False)\n",
    "    # when the value is pos (>=0), keep the value from real_mastorch. otherwise, replace with 0\n",
    "    real_mask = torch.where(real_mask>=0, real_mask, 0)\n",
    "\n",
    "    # calculates the intersection and union between real and predicted by using a log AND and OR functions from numpy\n",
    "    intersection = np.logical_and(real_mask, predicted_mask)\n",
    "    union = np.logical_or(real_mask, predicted_mask)\n",
    "\n",
    "    # if there is no object in either mask (both are entirely 0s), return 1 since IoU for \n",
    "    # empty masks would be perfect\n",
    "    if np.sum(union) == 0:\n",
    "        return 1\n",
    "    \n",
    "    # else, calculate and return intersection over union (IoU)\n",
    "    return np.sum(intersection) / np.sum(union)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculation of recall metric.\n",
    "    \n",
    "Args:\n",
    "    real_mask (Tensor): Ground-truth mask\n",
    "    predicted_mask (Tensor): Mask predicted by model\n",
    "Returns:\n",
    "    (float): recall metric value\n",
    "\n",
    "CHANGES:\n",
    "    - changed tf to torch\n",
    "\"\"\"\n",
    "def recall_metric(real_mask: torch.Tensor, predicted_mask: torch.Tensor) -> float:\n",
    "\n",
    "    real_mask = torch.where(real_mask < 0, 0, real_mask)\n",
    "    \n",
    "    true_positives = np.sum(np.logical_and(real_mask, predicted_mask))\n",
    "    actual_positives = np.sum(real_mask)\n",
    "    if actual_positives == 0:\n",
    "        return 1\n",
    "    \n",
    "    return true_positives / actual_positives\n",
    "\n",
    "\"\"\"\n",
    "Calculation of precision metric.\n",
    "    \n",
    "Args:\n",
    "    real_mask (Tensor): Ground-truth mask\n",
    "    predicted_mask (Tensor): Mask predicted by model\n",
    "Returns:\n",
    "    (float): precision metric value\n",
    "\n",
    "CHANGES:\n",
    "    - changed tf to torch\n",
    "\"\"\"\n",
    "def precision_metric(real_mask: torch.Tensor, predicted_mask: torch.Tensor) -> float:\n",
    "    real_mask = torch.where(real_mask < 0, 0, real_mask)\n",
    "    \n",
    "    true_positives = np.sum(np.logical_and(real_mask, predicted_mask))\n",
    "    predicted_positives = np.sum(predicted_mask)\n",
    "    if predicted_positives == 0:\n",
    "        return 1\n",
    "    \n",
    "    return true_positives / predicted_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss functions (DONE)\n",
    "- dice coefficient: metric used to evaluate similarity between sets, particularly in image segmentation. Dice coeff is calculated to be 2 times the intersection of the ground truth and predicted, over ground truth plus predicted. \n",
    "$$\n",
    "\\text{Dice Coefficient} = \\frac{2 \\times |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "\n",
    "- weighted binary cross entropy\n",
    "\n",
    "- BCE Dice loss \n",
    "regular dice loss:  \n",
    "$$ \\text{Dice Loss} = 1 - \\text{Dice Coefficient}$$  \n",
    "BCE dice loss adds dice loss and wBCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dice loss function calculator.\n",
    "    \n",
    "Args:\n",
    "    y_true (Tensor): \n",
    "    y_pred (Tensor):\n",
    "Returns:\n",
    "    (Tensor): Dice loss for each element of a batch.\n",
    "\n",
    "CHANGES:\n",
    "    - changed tf to torch in method header\n",
    "    - changed K to torch throughout\n",
    "\"\"\"\n",
    "def dice_coef(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    smooth = 1e-6\n",
    "    y_true_f = torch.reshape(y_true, (BATCH_SIZE, -1))\n",
    "    y_pred_f = torch.reshape(y_pred, (BATCH_SIZE, -1))\n",
    "    intersection = torch.sum(y_true_f * y_pred_f, axis=1)\n",
    "    return 1 - (2. * intersection + smooth) / (torch.sum(y_true_f, axis=1) + torch.sum(y_pred_f, axis=1) + smooth)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates weighted binary cross entropy. The weights are fixed.\n",
    "    \n",
    "This can be useful for unbalanced catagories.\n",
    "\n",
    "Adjust the weights here depending on what is required.\n",
    "\n",
    "For example if there are 10x as many positive classes as negative classes,\n",
    "    if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "    will be penalize 10 times as much as false negatives.\n",
    "\n",
    "Args:\n",
    "    true (Tensor): Ground-truth values\n",
    "    pred (Tensor): Predited values\n",
    "    weight_zero (float): Weight of class 0 (no-fire)\n",
    "    weight_one (float): Weight of class 1 (fire)\n",
    "\n",
    "Returns: \n",
    "    (float) : value for weighted binary cross entropy\n",
    "CHANGES:\n",
    "    - changed tf to torch in method header\n",
    "    - changed K to torch throughout\n",
    "    - changed keras BCE method to torch.nn.functional.binary_cross_entropy\n",
    "    \n",
    "\"\"\"\n",
    "def weighted_bincrossentropy(true: torch.Tensor, pred: torch.Tensor, weight_zero: float = 0.01, weight_one: float = 1) -> float:\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    # using torch.nn.functional.binary_cross_entropy, set reduction='none' to keep individual losses in a tensor\n",
    "    # rather than taking mean \n",
    "    bin_crossentropy = F.binary_cross_entropy(true, pred, reduction='none')\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "    \n",
    "    return torch.mean(weighted_bin_crossentropy, axis=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BCE loss function calculator.\n",
    "\n",
    "Args:\n",
    "    y_true (Tensor): \n",
    "    y_pred (Tensor):\n",
    "Returns:\n",
    "    (Tensor): Mean BCE Dice loss over a batch.\n",
    "\n",
    "CHANGES:\n",
    "    - changed tf to torch\n",
    "    - originally returned keras.reduce_weighted_loss(loss), but without additional args all that did was \n",
    "      perform a sum operation. Replaced it with torch.sum since there is no torch equivalent to reduce_weighted_loss\n",
    "\"\"\"\n",
    "def bce_dice_loss(y_true: torch.Tensor, y_pred: torch.Tensor):    \n",
    "    y_true_f = torch.reshape(y_true, (BATCH_SIZE, -1))\n",
    "    y_pred_f = torch.reshape(y_pred, (BATCH_SIZE, -1))\n",
    "    return torch.sum(weighted_bincrossentropy(y_true_f, y_pred_f) + dice_coef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads dataset according to file pattern and evaluates model's predictions on it.\n",
    "\n",
    "Parameters:\n",
    "    model (Callable[[tf.Tensor], tf.Tensor]): Function for model inference.\n",
    "    eval_dataset (tf.dataDataset): Dataset for evaluation.\n",
    "\n",
    "Returns:\n",
    "    Tuple[float, float, float, float]: IoU score, recall score, precision score and mean loss.\n",
    "\n",
    "CHANGES:\n",
    "    - changed tf to torch\n",
    "    - in method header, imported DataLoader from torch.utils and changed  eval_dataset: tf.data.Dataset) to DataLoader\n",
    "    - changed tf.expand_dims(tf.cast(predictions, tf.float32), axis=-1) to predictions.float().unsqueeze(-1)\n",
    "    in losses.append\n",
    "\"\"\"\n",
    "def evaluate_model(prediction_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "                   eval_dataset: DataLoader) -> Tuple[float, float, float, float]:\n",
    "    IoU_measures = []\n",
    "    recall_measures = []\n",
    "    precision_measures = []\n",
    "    losses = []\n",
    "    \n",
    "    for inputs, labels in tqdm(eval_dataset):\n",
    "        # Prediction shape (N, W, H)\n",
    "        predictions = prediction_function(inputs)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            IoU_measures.append(IoU_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "            recall_measures.append(recall_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "            precision_measures.append(precision_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "        labels_cleared = torch.where(labels < 0, 0, labels)\n",
    "        losses.append(bce_dice_loss(labels_cleared, predictions.float().unsqueeze(-1)\n",
    "))\n",
    "            \n",
    "    mean_IoU = np.mean(IoU_measures)\n",
    "    mean_recall = np.mean(recall_measures)\n",
    "    mean_precision = np.mean(precision_measures)\n",
    "    mean_loss = np.mean(losses)\n",
    "    return mean_IoU, mean_recall, mean_precision, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osgc-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
