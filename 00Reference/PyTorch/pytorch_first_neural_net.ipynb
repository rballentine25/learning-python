{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: A simple two layer fully connected network\n",
    "\n",
    "Here we will go step by step through implmenting a very simple 2 layer fully connected network to label MNIST digits.\n",
    "\n",
    "We start with a number of extra imports: pyplot lets us create plots, torch.nn is a torch library for implementing neural nets, and transforms is a torchvision package to help us work with image data, and datasets is a torchvision package granting us access to some built in test data. We will look at importing data in a later example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the pixel values to [-1,1] with mean 0.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "training_data = dsets.MNIST(root=\"./data\", train = True, transform=transform, download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a **validation set** of data as well, but the built in MNIST caller does not support a validation set, and we will just use the testing data set as the validation set for purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = dsets.MNIST(root=\"./data\", train = False, transform=transform, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2106d4ee080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGodJREFUeJzt3Q1wFGWex/H/hJcASpINkbxIwiagoAJZRcAsiFFSiXjH8nYeLFgFlgUFAiVkfbnsKS/uXkWxyrXkeLndUyJVCkqVwMFpLAwkKTSRBeSQ0s0RNkoiBBaukkCQEDJ99TSXkdFErodJ/pPp76eqazIz/U93Op359dP99BOPZVmWAADQySI6e4EAABgEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFR0lxDj9Xrl5MmT0rdvX/F4PNqrAwBwyIxvcP78eUlKSpKIiIiuE0AmfJKTk7VXAwBwg6qrq2XAgAFdJ4BMy8cYJ49Id+mhvToAAIeuSLPskw98n+edHkBr166VV155RWprayU9PV3WrFkjo0ePvm5d62k3Ez7dPQQQAHQ5/zfC6PUuo3RIJ4R3331XcnNzZcWKFXLo0CE7gHJycuTMmTMdsTgAQBfUIQH06quvyrx58+Txxx+XO++8UzZs2CB9+vSRN998syMWBwDogoIeQJcvX5aDBw9KVlbW9wuJiLCfl5WV/Wj+pqYmaWho8JsAAOEv6AF09uxZaWlpkfj4eL/XzXNzPeiH8vPzJTo62jfRAw4A3EH9RtS8vDypr6/3TabbHgAg/AW9F1xcXJx069ZNTp8+7fe6eZ6QkPCj+SMjI+0JAOAuQW8B9ezZU0aOHClFRUV+oxuY5xkZGcFeHACgi+qQ+4BMF+w5c+bIvffea9/789prr0ljY6PdKw4AgA4LoBkzZsjf/vY3Wb58ud3x4Be/+IUUFhb+qGMCAMC9PJYZNS6EmG7YpjdcpkxmJAQA6IKuWM1SLDvsjmVRUVGh2wsOAOBOBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFR011ks0LG6xUQHVNd0z2DHNVVTnf8ZVUxb57gmQjyOa7xiSSAe/OJRxzVR/3jWcU1LQ4PjGoQPWkAAABUEEAAgPAJo5cqV4vF4/KahQ4cGezEAgC6uQ64B3XXXXfLxxx9/v5DuXGoCAPjrkGQwgZOQkNAR3xoAECY65BrQsWPHJCkpSdLS0mT27Nly4sSJdudtamqShoYGvwkAEP6CHkBjxoyRgoICKSwslPXr10tVVZXcf//9cv78+Tbnz8/Pl+joaN+UnJwc7FUCALghgCZOnCiPPvqojBgxQnJycuSDDz6Quro6ee+999qcPy8vT+rr631TdXV1sFcJABCCOrx3QExMjNx+++1SWVnZ5vuRkZH2BABwlw6/D+jChQty/PhxSUxM7OhFAQDcHEBPP/20lJSUyNdffy2ffvqpTJ06Vbp16ya//vWvg70oAEAXFvRTcDU1NXbYnDt3Tm655RYZN26clJeX218DANBhAbRly5Zgf0u43ejhjksaXmwMaFFFwzc4rokI4ESCV7ydcsIisOWI7B2+1XHNxJHzHNd023vIcQ3CB2PBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQACM9/SAdc69iaMY5rKqatc1wTIR4JhDeAY7LTLd85rll37peOa6bEHHRcc3fPwI4xA9l+f53aw3HNbXsdlyCM0AICAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKhgNGx0qkBGtvaKt9OOrQJZ1tSVzziuiX2zzHHNr/4a0UnbznC+rLRtzQEuC25FCwgAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKBiNFp4oQT0BVnbMckcwvZnTKwKKntt/huGZU5CHHNd4AjzFXnLnbcU23vc7XD+5GCwgAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKBiNFpxr2p8XOiyznJb3OSUASNx11XONJHuC4ZsWd/+m4xhvAhvCKVwKxe81YxzWx4nxQVrgbLSAAgAoCCADQNQKotLRUJk2aJElJSeLxeGT79u1+71uWJcuXL5fExETp3bu3ZGVlybFjx4K5zgAANwZQY2OjpKeny9q1a9t8f/Xq1fL666/Lhg0b5LPPPpObbrpJcnJy5NKlS8FYXwCAWzshTJw40Z7aYlo/r732mjz//PMyefJk+7VNmzZJfHy83VKaOXPmja8xACAsBPUaUFVVldTW1tqn3VpFR0fLmDFjpKys7R4yTU1N0tDQ4DcBAMJfUAPIhI9hWjzXMs9b3/uh/Px8O6Rap+Tk5GCuEgAgRKn3gsvLy5P6+nrfVF1drb1KAICuFkAJCQn24+nTp/1eN89b3/uhyMhIiYqK8psAAOEvqAGUmppqB01RUZHvNXNNx/SGy8jICOaiAABu6wV34cIFqays9Ot4cPjwYYmNjZWUlBRZunSp/P73v5fbbrvNDqQXXnjBvmdoypQpwV53AICbAujAgQPy4IMP+p7n5ubaj3PmzJGCggJ59tln7XuF5s+fL3V1dTJu3DgpLCyUXr16BXfNAQBdmscyN++EEHPKzvSGy5TJ0t3TQ3t14DLfTR7tuObkjMuOa7564A3HNRHi6ZQBTI1RLy1xXJNUdNZxTcuX/+24BqHvitUsxbLD7lj2U9f11XvBAQDciQACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCADQNf4dA9AVdE8eEFDduJXljmt+Ff254xqveDvleDGw5Yj8+Z/WOK7Zv8z5aN2P7ZnvuObOlacc11yp+dZxDToeLSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqGIwUYemRj/4roLr50V87rokQ54NwegM49gtkOYEeYwayrNGRluOayol/dFyzLiPVcU3hw8MlEFeqawKqw/8PLSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqGIwUYSmQQUUNr3g75TgutJfTmctyvpz5MZWOa/5t1t9JIG59mcFIOxItIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoYjBRhKUI8AVc6Nb3S+UCXLbO7Oa65UvOthLLIkgTHNdsGfxDAkjhuDhf8JgEAKgggAEDXCKDS0lKZNGmSJCUlicfjke3bt/u9P3fuXPv1a6eHH344mOsMAHBjADU2Nkp6erqsXbu23XlM4Jw6dco3bd68+UbXEwDg9k4IEydOtKefEhkZKQkJzi9IAgDco0OuARUXF0v//v1lyJAhsnDhQjl37ly78zY1NUlDQ4PfBAAIf0EPIHP6bdOmTVJUVCQvv/yylJSU2C2mlpaWNufPz8+X6Oho35ScnBzsVQIAuOE+oJkzZ/q+Hj58uIwYMUIGDRpkt4omTJjwo/nz8vIkNzfX99y0gAghAAh/Hd4NOy0tTeLi4qSysrLd60VRUVF+EwAg/HV4ANXU1NjXgBITEzt6UQCAcD4Fd+HCBb/WTFVVlRw+fFhiY2PtadWqVTJ9+nS7F9zx48fl2WeflcGDB0tOTk6w1x0A4KYAOnDggDz44IO+563Xb+bMmSPr16+XI0eOyFtvvSV1dXX2zarZ2dnyu9/9zj7VBgBAwAGUmZkplmW1+/5HH33k9FsCQTfsT4sDK2x/125XyqpPA1sWxBvABveK13FNvy+vOK5Bx2MsOACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIABAePxLbiAUpKxkhOobceWhkY5rViX/0XFNhHgc18yvfshxTa+d+x3XoOPRAgIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCwUjRqbonD3BeZFmOS67UfOt8OfD5l393PrDo3ZFexzXeAI6Byz8c7rgmRRicNhTRAgIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCwUgRsO8mj3Zcs3fdBsc1Q4qfcFwzaPa3YTco61fP3uq4pmLaOglEhHg6ZWDRFWfudlyT9sY3jmuuOK5AZ6AFBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAWDkSJg/zPU+e7jFctxzVsZbziueXH0HAnI/i86ZWDRE69HOa6pGO18YFGveKWzjk0DWdYny+9zXNOrZr/jGoQmWkAAABUEEAAg9AMoPz9fRo0aJX379pX+/fvLlClTpKKiwm+eS5cuyaJFi6Rfv35y8803y/Tp0+X06dPBXm8AgJsCqKSkxA6X8vJy2b17tzQ3N0t2drY0Njb65lm2bJns3LlTtm7das9/8uRJmTZtWkesOwCgC3N0FbmwsNDveUFBgd0SOnjwoIwfP17q6+vljTfekHfeeUceeughe56NGzfKHXfcYYfWffc5v+AIAAhPN3QNyASOERsbaz+aIDKtoqysLN88Q4cOlZSUFCkrK2vzezQ1NUlDQ4PfBAAIfwEHkNfrlaVLl8rYsWNl2LBh9mu1tbXSs2dPiYmJ8Zs3Pj7efq+960rR0dG+KTk5OdBVAgC4IYDMtaCjR4/Kli1bbmgF8vLy7JZU61RdXX1D3w8AEMY3oi5evFh27dolpaWlMmDA9zfhJSQkyOXLl6Wurs6vFWR6wZn32hIZGWlPAAB3cdQCsizLDp9t27bJnj17JDU11e/9kSNHSo8ePaSoqMj3mummfeLECcnIyAjeWgMA3NUCMqfdTA+3HTt22PcCtV7XMdduevfubT8+8cQTkpuba3dMiIqKkiVLltjhQw84AEDAAbR+/Xr7MTMz0+9109V67ty59td/+MMfJCIiwr4B1fRwy8nJkXXrnI9hBQAIbx7LnFcLIaYbtmlJZcpk6e7pob06+AnfPvdLxzVHn3J+MNJstTiuiRCPBGL8F//guGZWyp8d18yP/rpTfqZABn8NdFn35i92XNP/Xz91XIPQd8VqlmLZYXcsM2fC2sNYcAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQACArvMfUQGj35dXOmVka694O+3Yas/wdwNYUkTI/kyBLUck84sZjmsSNx11XON8b0A4oQUEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABYORImC9du53XDMk+0nHNRXT1jmuiRCP45rWys5ZlvPlrDhzt+Oa/9gyTgJx68ufOq5hYFE4RQsIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgYjRaca+s9fOa4ZXbHEcU2fv6+VQDyafMhxzX29jzuuefwt5z9T2hvfOK65tcb5oKJAZ6EFBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQIXHsixLQkhDQ4NER0dLpkyW7p4e2qsDAHDoitUsxbJD6uvrJSoqqt35aAEBAFQQQACA0A+g/Px8GTVqlPTt21f69+8vU6ZMkYqKCr95MjMzxePx+E0LFiwI9noDANwUQCUlJbJo0SIpLy+X3bt3S3Nzs2RnZ0tjY6PffPPmzZNTp075ptWrVwd7vQEAbvqPqIWFhX7PCwoK7JbQwYMHZfz48b7X+/TpIwkJCcFbSwBA2Lmha0Cmh4MRGxvr9/rbb78tcXFxMmzYMMnLy5OLFy+2+z2amprsnm/XTgCA8OeoBXQtr9crS5culbFjx9pB02rWrFkycOBASUpKkiNHjshzzz1nXyd6//33272utGrVqkBXAwDgtvuAFi5cKB9++KHs27dPBgwY0O58e/bskQkTJkhlZaUMGjSozRaQmVqZFlBycjL3AQFAmN8HFFALaPHixbJr1y4pLS39yfAxxowZYz+2F0CRkZH2BABwF0cBZBpLS5YskW3btklxcbGkpqZet+bw4cP2Y2JiYuBrCQBwdwCZLtjvvPOO7Nixw74XqLa21n7dDJ3Tu3dvOX78uP3+I488Iv369bOvAS1btszuITdixIiO+hkAAOF+DcjcVNqWjRs3yty5c6W6uloee+wxOXr0qH1vkLmWM3XqVHn++ed/8jzgtRgLDgC6tg65BnS9rDKBY25WBQDgehgLDgCgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgoruEGMuy7Mcr0ixy9UsAQBdif35f83neZQLo/Pnz9uM++UB7VQAAN/h5Hh0d3e77Hut6EdXJvF6vnDx5Uvr27Ssej8fvvYaGBklOTpbq6mqJiooSt2I7XMV2uIrtcBXbIXS2g4kVEz5JSUkSERHRdVpAZmUHDBjwk/OYjermHawV2+EqtsNVbIer2A6hsR1+quXTik4IAAAVBBAAQEWXCqDIyEhZsWKF/ehmbIer2A5XsR2uYjt0ve0Qcp0QAADu0KVaQACA8EEAAQBUEEAAABUEEABARZcJoLVr18rPf/5z6dWrl4wZM0b2798vbrNy5Up7dIhrp6FDh0q4Ky0tlUmTJtl3VZufefv27X7vm340y5cvl8TEROndu7dkZWXJsWPHxG3bYe7cuT/aPx5++GEJJ/n5+TJq1Ch7pJT+/fvLlClTpKKiwm+eS5cuyaJFi6Rfv35y8803y/Tp0+X06dPitu2QmZn5o/1hwYIFEkq6RAC9++67kpuba3ctPHTokKSnp0tOTo6cOXNG3Oauu+6SU6dO+aZ9+/ZJuGtsbLR/5+YgpC2rV6+W119/XTZs2CCfffaZ3HTTTfb+YT6I3LQdDBM41+4fmzdvlnBSUlJih0t5ebns3r1bmpubJTs72942rZYtWyY7d+6UrVu32vObob2mTZsmbtsOxrx58/z2B/O3ElKsLmD06NHWokWLfM9bWlqspKQkKz8/33KTFStWWOnp6ZabmV1227Ztvuder9dKSEiwXnnlFd9rdXV1VmRkpLV582bLLdvBmDNnjjV58mTLTc6cOWNvi5KSEt/vvkePHtbWrVt983z11Vf2PGVlZZZbtoPxwAMPWE899ZQVykK+BXT58mU5ePCgfVrl2vHizPOysjJxG3NqyZyCSUtLk9mzZ8uJEyfEzaqqqqS2ttZv/zBjUJnTtG7cP4qLi+1TMkOGDJGFCxfKuXPnJJzV19fbj7Gxsfaj+awwrYFr9wdzmjolJSWs94f6H2yHVm+//bbExcXJsGHDJC8vTy5evCihJOQGI/2hs2fPSktLi8THx/u9bp7/5S9/ETcxH6oFBQX2h4tpTq9atUruv/9+OXr0qH0u2I1M+Bht7R+t77mFOf1mTjWlpqbK8ePH5be//a1MnDjR/uDt1q2bhBszcv7SpUtl7Nix9gesYX7nPXv2lJiYGNfsD942toMxa9YsGThwoH3AeuTIEXnuuefs60Tvv/++hIqQDyB8z3yYtBoxYoQdSGYHe++99+SJJ55QXTfomzlzpu/r4cOH2/vIoEGD7FbRhAkTJNyYayDm4MsN10ED2Q7z58/32x9MJx2zH5iDE7NfhIKQPwVnmo/m6O2HvVjM84SEBHEzc5R3++23S2VlpbhV6z7A/vFj5jSt+fsJx/1j8eLFsmvXLtm7d6/fv28xv3Nz2r6urs4V+8PidrZDW8wBqxFK+0PIB5BpTo8cOVKKior8mpzmeUZGhrjZhQsX7KMZc2TjVuZ0k/lguXb/MP+Qy/SGc/v+UVNTY18DCqf9w/S/MB+627Ztkz179ti//2uZz4oePXr47Q/mtJO5VhpO+4N1ne3QlsOHD9uPIbU/WF3Ali1b7F5NBQUF1pdffmnNnz/fiomJsWpray03+c1vfmMVFxdbVVVV1ieffGJlZWVZcXFxdg+YcHb+/Hnr888/tyezy7766qv219988439/ksvvWTvDzt27LCOHDli9wRLTU21vvvuO8st28G89/TTT9s9vcz+8fHHH1v33HOPddttt1mXLl2ywsXChQut6Oho++/g1KlTvunixYu+eRYsWGClpKRYe/bssQ4cOGBlZGTYUzhZeJ3tUFlZab344ov2z2/2B/O3kZaWZo0fP94KJV0igIw1a9bYO1XPnj3tbtnl5eWW28yYMcNKTEy0t8Gtt95qPzc7Wrjbu3ev/YH7w8l0O27tiv3CCy9Y8fHx9oHKhAkTrIqKCstN28F88GRnZ1u33HKL3Q154MCB1rx588LuIK2tn99MGzdu9M1jDjyefPJJ62c/+5nVp08fa+rUqfaHs5u2w4kTJ+ywiY2Ntf8mBg8ebD3zzDNWfX29FUr4dwwAABUhfw0IABCeCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAiIb/BbxE6xhMRtpWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The mnist data comes in as a tuple. The first index is the pixel values packed in a pytorch Tensor, \n",
    "## the 2nd index is the class label.\n",
    "# To show the image I have to make pytorch tensor a \n",
    "training_image_tensor = training_data[890][0]\n",
    "\n",
    "# Note that the image is a 1x28x28 tensor: there is only one color channel; it is grayscale. \n",
    "print(training_image_tensor.shape)\n",
    "plt.imshow(training_image_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "class_label = training_data[890][1]\n",
    "print(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch comes with **built-in DataLoaders** that make it trivial to create minibatches and to enumerate over them. Always use these helpers!!\n",
    "\n",
    "A DataLoader takes as input the desired minibatch size you would like (batch_size), and whether or not you would like the data to be shuffled before it is partitioned into minibatches. No shuffling means the exact same minibatches will be presented to your network every epoch. There is not a lot of guidance as to whether shuffling is good or bad, but intuition says shuffling should help with generalizability a bit.\n",
    "\n",
    "The DataLoader function returns an *iterator* over your datasets with shuffling and batchsize specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data, batch_size=len(testing_data), shuffle=False)\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a deep net\n",
    "### 1. Specify and instantiate the network.\n",
    "\n",
    "Here we go. A DNN is specified as a class that inherets from nn.Module. They all have the same structure:\n",
    "<ol>\n",
    "    <li> a call to the constructor of nn.Module (super)\n",
    "    <li> specification of a number of network layers. It is common practice to wrap the way the nodes are wired and the activation function (and any other processing, like doing batch norm) inside of an nn.Sequential object. nn.Sequential is just a convinent wrapper of a number of nn.function calls. \n",
    "    <li> Definition of a **forward** function. This function must be specified. This function describes the way that data will pass through your deep net. Here, we see that the input will bass through fc1, then fc2, and we return thr output of fc2. \n",
    "        </ol>\n",
    "        \n",
    "A few things to note: \n",
    "<ol> \n",
    "    <li> We do not need to tell the deep net what the size of the minibatch we will present to it is.\n",
    "    <li> nn.Linear specifies a linear combination of inputs into a layer. The linear combination is the $\\sum_i w_i x_i$ we are used to seeing, where the summation is over **every node in the previous layer**. So nn.Linear specifies a fully-connected connectivity in this layer. \n",
    "<li> We simply output the ReLU activation values of fc2 -- we do not normalize these values so they sum to 1. We may want to do a normalization here if we want the network to output a score for the likelihood the input is of each class that looks like a probability. We could do so by specifiying a **softmax** layer. But this is equivalent to just predicting the class corresponding to the fc2 output that is maximum.\n",
    "    </ol>\n",
    "    \n",
    "\n",
    "**The PyTorch docs are a great place to see what types of layers and activations are implemented**. Of course, nothing is stoping you from implementing your own layer or activation function and then call those functions inside of the deep net specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(DeepNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[-0.0236,  0.0214, -0.0319,  ..., -0.0040,  0.0114, -0.0301],\n",
      "        [-0.0278, -0.0142, -0.0270,  ..., -0.0117, -0.0211, -0.0269],\n",
      "        [ 0.0335,  0.0127,  0.0172,  ...,  0.0111, -0.0168, -0.0119],\n",
      "        ...,\n",
      "        [-0.0132, -0.0237, -0.0084,  ..., -0.0007,  0.0120,  0.0231],\n",
      "        [-0.0191,  0.0302,  0.0221,  ...,  0.0011, -0.0002, -0.0291],\n",
      "        [ 0.0331,  0.0301, -0.0283,  ...,  0.0329, -0.0138,  0.0002]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0008, -0.0053,  0.0069,  0.0133, -0.0057, -0.0177,  0.0060,  0.0292,\n",
      "        -0.0132,  0.0092], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0938, -0.2699,  0.1675, -0.1279,  0.1713, -0.3156, -0.2380, -0.1302,\n",
      "         -0.0207, -0.0630],\n",
      "        [ 0.2726, -0.1411, -0.1427, -0.2106, -0.0819, -0.1788, -0.0168, -0.1927,\n",
      "         -0.0841, -0.2577],\n",
      "        [-0.1471, -0.1418,  0.2572,  0.0943, -0.0727,  0.0879, -0.1543, -0.1495,\n",
      "         -0.2395, -0.1529],\n",
      "        [ 0.1961, -0.1678, -0.2335, -0.2818, -0.2754, -0.2158,  0.1085, -0.3094,\n",
      "          0.0339, -0.0805],\n",
      "        [-0.0404,  0.1607,  0.0432, -0.1054, -0.1363,  0.2693, -0.1841,  0.2227,\n",
      "         -0.1372,  0.0894],\n",
      "        [ 0.1784, -0.0023, -0.0662,  0.1987,  0.1322, -0.3042, -0.0800, -0.2347,\n",
      "         -0.2417, -0.2131],\n",
      "        [-0.0052,  0.0762, -0.1803, -0.3054, -0.2968,  0.2756, -0.1222,  0.1355,\n",
      "         -0.1615, -0.1514],\n",
      "        [ 0.2109, -0.0611,  0.1253, -0.2993, -0.2719, -0.1204, -0.2222,  0.1640,\n",
      "         -0.2838,  0.1059],\n",
      "        [ 0.1547, -0.2601,  0.0850,  0.2919,  0.1461,  0.0639, -0.1958,  0.2026,\n",
      "         -0.2956,  0.0158],\n",
      "        [-0.2494,  0.1179, -0.1764, -0.1131,  0.2479,  0.1868,  0.0471, -0.2814,\n",
      "         -0.2423,  0.0890]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1095, -0.0762, -0.0438, -0.0316,  0.2095,  0.2759,  0.1533, -0.2615,\n",
      "         0.2175, -0.2754], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1395, -0.2338, -0.2991, -0.1967, -0.0310, -0.2569,  0.0037, -0.0147,\n",
      "          0.2242,  0.2156],\n",
      "        [-0.0191, -0.1215,  0.1545,  0.0403,  0.2554, -0.1999,  0.0093, -0.1051,\n",
      "         -0.0126,  0.0953],\n",
      "        [ 0.1195,  0.1485, -0.1018, -0.2702,  0.0018,  0.2667,  0.1397, -0.0840,\n",
      "          0.0003,  0.0470],\n",
      "        [ 0.1700, -0.1562, -0.1605,  0.0358,  0.2210,  0.0615, -0.0180, -0.2217,\n",
      "         -0.1503, -0.1403],\n",
      "        [ 0.1554,  0.1829, -0.2360, -0.1161, -0.3091,  0.0853, -0.1395, -0.2067,\n",
      "          0.2790, -0.2192],\n",
      "        [-0.1097,  0.2514, -0.1103,  0.0489, -0.1333, -0.2290, -0.1599, -0.1454,\n",
      "          0.2536,  0.1349],\n",
      "        [-0.0523,  0.1637,  0.2183,  0.1592, -0.0686,  0.2835,  0.0119,  0.2757,\n",
      "         -0.1464, -0.0905],\n",
      "        [-0.0013, -0.2815,  0.0129, -0.2116,  0.1120,  0.1390, -0.0249, -0.2849,\n",
      "          0.1159, -0.1664],\n",
      "        [ 0.2904, -0.0801, -0.1642, -0.1150,  0.0403, -0.1563,  0.0193, -0.0941,\n",
      "          0.1416, -0.2565],\n",
      "        [ 0.0832, -0.1153,  0.2758,  0.1604, -0.0128, -0.2462, -0.0048, -0.1087,\n",
      "         -0.1521,  0.3001]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0726, -0.1042,  0.2285, -0.1126, -0.2528,  0.2548,  0.2481,  0.3127,\n",
      "        -0.1399, -0.1772], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.0236,  0.0214, -0.0319,  ..., -0.0040,  0.0114, -0.0301],\n",
      "        [-0.0278, -0.0142, -0.0270,  ..., -0.0117, -0.0211, -0.0269],\n",
      "        [ 0.0335,  0.0127,  0.0172,  ...,  0.0111, -0.0168, -0.0119],\n",
      "        ...,\n",
      "        [-0.0132, -0.0237, -0.0084,  ..., -0.0007,  0.0120,  0.0231],\n",
      "        [-0.0191,  0.0302,  0.0221,  ...,  0.0011, -0.0002, -0.0291],\n",
      "        [ 0.0331,  0.0301, -0.0283,  ...,  0.0329, -0.0138,  0.0002]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0008, -0.0053,  0.0069,  0.0133, -0.0057, -0.0177,  0.0060,  0.0292,\n",
      "        -0.0132,  0.0092], requires_grad=True), Parameter containing:\n",
      "tensor([[ 9.9923,  9.9977, 10.0012, 10.0146, 10.0003,  9.9962, 10.0124,  9.9981,\n",
      "         10.0013, 10.0160],\n",
      "        [10.0125, 10.0036,  9.9957,  9.9779,  9.9779,  9.9925,  9.9931, 10.0040,\n",
      "         10.0073, 10.0162],\n",
      "        [ 9.9871, 10.0029,  9.9994,  9.9714,  9.9994,  9.9963,  9.9978, 10.0034,\n",
      "          9.9969, 10.0002],\n",
      "        [ 9.9990,  9.9992, 10.0056, 10.0054,  9.9997,  9.9965, 10.0060,  9.9975,\n",
      "         10.0160, 10.0037],\n",
      "        [ 9.9946, 10.0064,  9.9925, 10.0063,  9.9960, 10.0102,  9.9958,  9.9823,\n",
      "         10.0118,  9.9918],\n",
      "        [10.0058,  9.9934, 10.0101,  9.9971,  9.9977, 10.0002, 10.0025, 10.0014,\n",
      "          9.9867, 10.0021],\n",
      "        [ 9.9991, 10.0006,  9.9837, 10.0064, 10.0096,  9.9956, 10.0102,  9.9795,\n",
      "          9.9803, 10.0022],\n",
      "        [ 9.9927, 10.0158,  9.9977, 10.0010, 10.0043, 10.0139,  9.9987, 10.0032,\n",
      "         10.0038, 10.0198],\n",
      "        [10.0026, 10.0016,  9.9711, 10.0152, 10.0010,  9.9997,  9.9801, 10.0170,\n",
      "          9.9948,  9.9948],\n",
      "        [ 9.9955,  9.9958,  9.9826,  9.9869,  9.9961, 10.0149,  9.9922, 10.0156,\n",
      "         10.0091,  9.9929]], requires_grad=True), Parameter containing:\n",
      "tensor([49.9754, 50.0014, 49.9669, 49.9826, 50.0149, 49.9990, 49.9888, 49.9994,\n",
      "        50.0000, 49.9939], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1395, -0.2338, -0.2991, -0.1967, -0.0310, -0.2569,  0.0037, -0.0147,\n",
      "          0.2242,  0.2156],\n",
      "        [-0.0191, -0.1215,  0.1545,  0.0403,  0.2554, -0.1999,  0.0093, -0.1051,\n",
      "         -0.0126,  0.0953],\n",
      "        [ 0.1195,  0.1485, -0.1018, -0.2702,  0.0018,  0.2667,  0.1397, -0.0840,\n",
      "          0.0003,  0.0470],\n",
      "        [ 0.1700, -0.1562, -0.1605,  0.0358,  0.2210,  0.0615, -0.0180, -0.2217,\n",
      "         -0.1503, -0.1403],\n",
      "        [ 0.1554,  0.1829, -0.2360, -0.1161, -0.3091,  0.0853, -0.1395, -0.2067,\n",
      "          0.2790, -0.2192],\n",
      "        [-0.1097,  0.2514, -0.1103,  0.0489, -0.1333, -0.2290, -0.1599, -0.1454,\n",
      "          0.2536,  0.1349],\n",
      "        [-0.0523,  0.1637,  0.2183,  0.1592, -0.0686,  0.2835,  0.0119,  0.2757,\n",
      "         -0.1464, -0.0905],\n",
      "        [-0.0013, -0.2815,  0.0129, -0.2116,  0.1120,  0.1390, -0.0249, -0.2849,\n",
      "          0.1159, -0.1664],\n",
      "        [ 0.2904, -0.0801, -0.1642, -0.1150,  0.0403, -0.1563,  0.0193, -0.0941,\n",
      "          0.1416, -0.2565],\n",
      "        [ 0.0832, -0.1153,  0.2758,  0.1604, -0.0128, -0.2462, -0.0048, -0.1087,\n",
      "         -0.1521,  0.3001]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0726, -0.1042,  0.2285, -0.1126, -0.2528,  0.2548,  0.2481,  0.3127,\n",
      "        -0.1399, -0.1772], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0068, -0.0106,  0.0136,  ..., -0.0351,  0.0282,  0.0156],\n",
      "        [-0.0219, -0.0297, -0.0280,  ..., -0.0207, -0.0025, -0.0126],\n",
      "        [-0.0300, -0.0084,  0.0179,  ..., -0.0164,  0.0148,  0.0108],\n",
      "        ...,\n",
      "        [ 0.0348,  0.0008,  0.0242,  ..., -0.0109, -0.0083,  0.0202],\n",
      "        [ 0.0291,  0.0342,  0.0024,  ..., -0.0312, -0.0339, -0.0209],\n",
      "        [ 0.0035,  0.0072,  0.0109,  ..., -0.0271, -0.0335, -0.0005]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0246, -0.0042, -0.0341,  0.0131,  0.0064,  0.0099,  0.0049,  0.0198,\n",
      "        -0.0121, -0.0222], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1277, -0.0626,  0.0870, -0.2319,  0.1158, -0.2250,  0.1552,  0.0407,\n",
      "          0.0337,  0.0166],\n",
      "        [ 0.1425,  0.3046,  0.0538, -0.1636, -0.0390,  0.2381, -0.3037,  0.2158,\n",
      "         -0.1175, -0.0357],\n",
      "        [-0.1220,  0.1947, -0.3064, -0.2755, -0.1952,  0.2965,  0.0187, -0.0767,\n",
      "          0.3148, -0.2960],\n",
      "        [ 0.0668,  0.2328,  0.0343, -0.2447,  0.2389,  0.0534, -0.3086, -0.2714,\n",
      "         -0.0307,  0.0373],\n",
      "        [ 0.2678,  0.0560,  0.2744, -0.0116,  0.1845, -0.0445,  0.1639, -0.2430,\n",
      "         -0.2162, -0.1733],\n",
      "        [-0.1652,  0.0422, -0.1775,  0.2044,  0.0201, -0.1984,  0.0187,  0.2352,\n",
      "         -0.2772,  0.0883],\n",
      "        [-0.0036, -0.0828,  0.0807, -0.1343, -0.2710,  0.1507,  0.2152,  0.1722,\n",
      "          0.2810,  0.0283],\n",
      "        [-0.1867,  0.0873,  0.0072,  0.0773,  0.1110, -0.1393,  0.0195,  0.1136,\n",
      "         -0.1384,  0.1337],\n",
      "        [-0.0213, -0.2366, -0.1711,  0.1280,  0.2073,  0.1086, -0.1833,  0.1809,\n",
      "          0.1856, -0.0803],\n",
      "        [-0.1391,  0.0687, -0.1364, -0.0713,  0.2584,  0.2794, -0.0617, -0.0238,\n",
      "         -0.2900, -0.1158]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2478, -0.1382,  0.0903, -0.2509,  0.0241, -0.0818, -0.2059, -0.1407,\n",
      "         0.0213,  0.1586], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2343,  0.0120, -0.0502, -0.0587,  0.0962,  0.3141,  0.1137, -0.0736,\n",
      "          0.1044,  0.0333],\n",
      "        [-0.1038,  0.2063,  0.0188,  0.1370, -0.1711,  0.1365, -0.0520,  0.1431,\n",
      "          0.0022,  0.2889],\n",
      "        [ 0.2294, -0.1198, -0.0914, -0.2714, -0.2478, -0.0234, -0.0119,  0.2192,\n",
      "          0.1786,  0.2070],\n",
      "        [ 0.1936, -0.0399, -0.1422, -0.0469,  0.1500, -0.0379,  0.2933, -0.3142,\n",
      "          0.3127, -0.0134],\n",
      "        [ 0.3118,  0.1516,  0.3089,  0.0591, -0.2359,  0.0611, -0.1595, -0.2023,\n",
      "         -0.3003, -0.2795],\n",
      "        [ 0.0137, -0.1649,  0.2403,  0.2338, -0.2484, -0.1594,  0.2510,  0.1361,\n",
      "         -0.0918, -0.0930],\n",
      "        [ 0.2383, -0.1931, -0.1550, -0.0637,  0.2840, -0.2798,  0.1431, -0.1131,\n",
      "          0.1455,  0.0846],\n",
      "        [ 0.2503, -0.0970,  0.2843, -0.1602,  0.2139,  0.0201, -0.0349,  0.2694,\n",
      "          0.0268,  0.1844],\n",
      "        [ 0.2212,  0.3031,  0.1849,  0.2261,  0.2314, -0.0286,  0.2169, -0.1226,\n",
      "          0.2389, -0.2597],\n",
      "        [ 0.1628, -0.2581,  0.0376, -0.1154, -0.1022,  0.2687, -0.0367,  0.1051,\n",
      "         -0.0683,  0.2365]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0739, -0.2545,  0.0328, -0.0371,  0.1959, -0.1679, -0.0439,  0.2661,\n",
      "        -0.1332, -0.1210], requires_grad=True)]\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "number of network parameters: 8070\n"
     ]
    }
   ],
   "source": [
    "input_size = 28*28 ## Note that an MNIST image is 28*28 pixels.\n",
    "num_classes = 10\n",
    "hidden_size = 10\n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(the_net)\n",
    "\n",
    "## the parameters() function of an nn.Module object returns an iterator over \n",
    "## all of the network parameters (e.g. weights.) PyTorch initializes all weights in linear and\n",
    "## conv layers as: \n",
    "## stdv = 1. / math.sqrt(number_of_layer_inputs)\n",
    "## self.weight.data.uniform_(-stdv, stdv)\n",
    "## So with uniformly distributed weights within -stdv, stdv.\n",
    "\n",
    "## If you want to see what the parameters are, iterate over parameters() or make it a list:\n",
    "params = list(the_net.parameters())\n",
    "## We print the parameters of the network in order of weights, then biases, for each layer that has weights and \n",
    "## biases, in sequential order of operation in the forward pass. \n",
    "print(params)\n",
    "\n",
    "## If you want to specify your own weight initiaization routine, you can do it simply!\n",
    "## It is possible to . through the structure of the_net. any Sequential layers are lists, so fc2[0] is the \n",
    "## nn.Linear in fc2. fc2[1] is the ReLU activation.\n",
    "## Remember that _ suffix means in place updating.\n",
    "the_net.fc2[0].weight.data.normal_(10,0.01)\n",
    "the_net.fc2[0].bias.data.normal_(50,0.01)\n",
    "print(params)\n",
    "\n",
    "## anyway, these weights are horrible. Let's reinitialize the network. :) \n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(list(the_net.parameters()))\n",
    "\n",
    "##BTW, how many parameters do we have? \n",
    "num_parameters = 0\n",
    "for x in the_net.parameters():\n",
    "    print(x.shape)\n",
    "for x in the_net.parameters():\n",
    "    num_params = 1\n",
    "    for dim_size in x.shape:\n",
    "        num_params *= dim_size\n",
    "    num_parameters += num_params\n",
    "print('number of network parameters: %i' % (num_parameters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define your loss function.\n",
    "\n",
    "Since we are tackling a classification problem we want to use cross entropy loss. For those who know cross entropy (or have heard of **entropy** before) would know that the loss must operate on a probability distribution. And indeed it does: this loss expects the probability the network thinks the input is of each class. In PyTorch, **this normalization (e.g. passing the output of fc2 through a *softmax layer*) is done automatically by the loss function**. I (Derek) don't really like this, its a bit of a gotcha! Now you know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define your optimizer.\n",
    "\n",
    "When in doubt, start with SGD with a small learning rate parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(the_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training.\n",
    "We will train in a loop that iterates over the minibatches yielded from train_loader, for a fixed number of epochs. Here is what's going on: \n",
    "<ol>\n",
    "    <li> We are specifying a loop for 10 epochs. In the loop we wrap the train_loader in an enumerator. The enumerator will yield an enumeration count and a tuple (minibatch_of_images, minibatch_of_labels). \n",
    "    <li> The minibatch_of_images is a 4D tensor with dimensions (size_of_minibatch, depth, width, height). The network expects as input a vector of size 28*28 so we need to reshape the tensor accordingly. We will reshape it to a matrix of 32 rows, 28*28 columns. Note that we can pass -1 as the first parameter of view, this is like saying \"I don't know how many rows I'll need, just make as many as needed to the column count I specify (28*28) is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 784]' is invalid for input of size 50176",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ , (minibatch_of_images, minibatch_of_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(minibatch_of_images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mminibatch_of_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 784]' is invalid for input of size 50176"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for _ , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "        print(minibatch_of_images.shape)\n",
    "        print(minibatch_of_images.view(32, 28*28).shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> We build a computation graph here to setup backprop! So wrap the labels and the batch in a Variable.\n",
    "    <li> Tell the optimizer to zero out any gradient stored in the Variables of the parameters of the_net. Indeed, the optimizer knows of the_net's parameters as we passed them in as a required parameter of the optimizer above. **Zero the gradient out after every minibatch!**\n",
    "     <li> Okay, run the_batch through the_net. This is the **forward pass**. Note that outputs will be a Variable.\n",
    "     <li> Compute the loss, how much error the network had on the_batch.\n",
    "     <li> Now backprop! loss is a real number value, so that will be the gradient signal. Observe. loss is a functino of the outputs, which is a function of the_net, which is a function of the_batch. So the computation chain is all setup! \n",
    "      <li> Tell the optimizer to update the weights by calling step(). Step tells the optimizer to use the gradients stored at each network parameter (computed by calling backward() on loss) to compute and apply a weight update.\n",
    "      <li> Give some console output to see how we are doing every few minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, minibatch 0. Loss: 2.2961.\n",
      "At epoch 0, minibatch 300. Loss: 1.7185.\n",
      "At epoch 0, minibatch 600. Loss: 1.2413.\n",
      "At epoch 0, minibatch 900. Loss: 0.5460.\n",
      "Epoch 0 finished! It took: 4.6194 seconds\n",
      "Accuracy of the network on the 10000 test images: 75.04 %\n",
      "At epoch 1, minibatch 0. Loss: 0.7775.\n",
      "At epoch 1, minibatch 300. Loss: 0.7185.\n",
      "At epoch 1, minibatch 600. Loss: 0.5047.\n",
      "At epoch 1, minibatch 900. Loss: 0.4089.\n",
      "Epoch 1 finished! It took: 4.5623 seconds\n",
      "Accuracy of the network on the 10000 test images: 84.66 %\n",
      "At epoch 2, minibatch 0. Loss: 0.4307.\n",
      "At epoch 2, minibatch 300. Loss: 0.3394.\n",
      "At epoch 2, minibatch 600. Loss: 0.6411.\n",
      "At epoch 2, minibatch 900. Loss: 0.4371.\n",
      "Epoch 2 finished! It took: 4.5404 seconds\n",
      "Accuracy of the network on the 10000 test images: 87.21 %\n",
      "At epoch 3, minibatch 0. Loss: 0.2563.\n",
      "At epoch 3, minibatch 300. Loss: 0.3732.\n",
      "At epoch 3, minibatch 600. Loss: 0.3172.\n",
      "At epoch 3, minibatch 900. Loss: 0.5036.\n",
      "Epoch 3 finished! It took: 4.4547 seconds\n",
      "Accuracy of the network on the 10000 test images: 89.06 %\n",
      "At epoch 4, minibatch 0. Loss: 0.2614.\n",
      "At epoch 4, minibatch 300. Loss: 0.3865.\n",
      "At epoch 4, minibatch 600. Loss: 0.4162.\n",
      "At epoch 4, minibatch 900. Loss: 0.2331.\n",
      "Epoch 4 finished! It took: 4.3392 seconds\n",
      "Accuracy of the network on the 10000 test images: 89.82 %\n",
      "At epoch 5, minibatch 0. Loss: 0.2301.\n",
      "At epoch 5, minibatch 300. Loss: 0.4133.\n",
      "At epoch 5, minibatch 600. Loss: 0.3251.\n",
      "At epoch 5, minibatch 900. Loss: 0.3148.\n",
      "Epoch 5 finished! It took: 4.3849 seconds\n",
      "Accuracy of the network on the 10000 test images: 90.40 %\n",
      "At epoch 6, minibatch 0. Loss: 0.3943.\n",
      "At epoch 6, minibatch 300. Loss: 0.3342.\n",
      "At epoch 6, minibatch 600. Loss: 0.1992.\n",
      "At epoch 6, minibatch 900. Loss: 0.2813.\n",
      "Epoch 6 finished! It took: 4.5371 seconds\n",
      "Accuracy of the network on the 10000 test images: 90.93 %\n",
      "At epoch 7, minibatch 0. Loss: 0.2425.\n",
      "At epoch 7, minibatch 300. Loss: 0.3049.\n",
      "At epoch 7, minibatch 600. Loss: 0.2383.\n",
      "At epoch 7, minibatch 900. Loss: 0.3119.\n",
      "Epoch 7 finished! It took: 4.5287 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.46 %\n",
      "At epoch 8, minibatch 0. Loss: 0.2332.\n",
      "At epoch 8, minibatch 300. Loss: 0.1041.\n",
      "At epoch 8, minibatch 600. Loss: 0.4495.\n",
      "At epoch 8, minibatch 900. Loss: 0.2495.\n",
      "Epoch 8 finished! It took: 4.4040 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.73 %\n",
      "At epoch 9, minibatch 0. Loss: 0.3718.\n",
      "At epoch 9, minibatch 300. Loss: 0.2458.\n",
      "At epoch 9, minibatch 600. Loss: 0.2475.\n",
      "At epoch 9, minibatch 900. Loss: 0.3793.\n",
      "Epoch 9 finished! It took: 4.3755 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.58 %\n",
      "At epoch 10, minibatch 0. Loss: 0.3887.\n",
      "At epoch 10, minibatch 300. Loss: 0.3244.\n",
      "At epoch 10, minibatch 600. Loss: 0.2583.\n",
      "At epoch 10, minibatch 900. Loss: 0.1777.\n",
      "Epoch 10 finished! It took: 4.3390 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.75 %\n",
      "At epoch 11, minibatch 0. Loss: 0.2252.\n",
      "At epoch 11, minibatch 300. Loss: 0.2825.\n",
      "At epoch 11, minibatch 600. Loss: 0.2154.\n",
      "At epoch 11, minibatch 900. Loss: 0.3208.\n",
      "Epoch 11 finished! It took: 4.2882 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.73 %\n",
      "At epoch 12, minibatch 0. Loss: 0.2005.\n",
      "At epoch 12, minibatch 300. Loss: 0.1785.\n",
      "At epoch 12, minibatch 600. Loss: 0.4926.\n",
      "At epoch 12, minibatch 900. Loss: 0.5239.\n",
      "Epoch 12 finished! It took: 4.4413 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.85 %\n",
      "At epoch 13, minibatch 0. Loss: 0.1928.\n",
      "At epoch 13, minibatch 300. Loss: 0.1278.\n",
      "At epoch 13, minibatch 600. Loss: 0.2717.\n",
      "At epoch 13, minibatch 900. Loss: 0.2045.\n",
      "Epoch 13 finished! It took: 4.4405 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.62 %\n",
      "At epoch 14, minibatch 0. Loss: 0.1490.\n",
      "At epoch 14, minibatch 300. Loss: 0.3115.\n",
      "At epoch 14, minibatch 600. Loss: 0.2652.\n",
      "At epoch 14, minibatch 900. Loss: 0.4750.\n",
      "Epoch 14 finished! It took: 4.5309 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.60 %\n",
      "At epoch 15, minibatch 0. Loss: 0.4221.\n",
      "At epoch 15, minibatch 300. Loss: 0.0464.\n",
      "At epoch 15, minibatch 600. Loss: 0.2730.\n",
      "At epoch 15, minibatch 900. Loss: 0.3781.\n",
      "Epoch 15 finished! It took: 4.4125 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.72 %\n",
      "At epoch 16, minibatch 0. Loss: 0.2190.\n",
      "At epoch 16, minibatch 300. Loss: 0.3725.\n",
      "At epoch 16, minibatch 600. Loss: 0.4068.\n",
      "At epoch 16, minibatch 900. Loss: 0.2504.\n",
      "Epoch 16 finished! It took: 4.4232 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.78 %\n",
      "At epoch 17, minibatch 0. Loss: 0.1778.\n",
      "At epoch 17, minibatch 300. Loss: 0.1521.\n",
      "At epoch 17, minibatch 600. Loss: 0.2025.\n",
      "At epoch 17, minibatch 900. Loss: 0.1768.\n",
      "Epoch 17 finished! It took: 4.4719 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.80 %\n",
      "At epoch 18, minibatch 0. Loss: 0.2701.\n",
      "At epoch 18, minibatch 300. Loss: 0.0878.\n",
      "At epoch 18, minibatch 600. Loss: 0.2174.\n",
      "At epoch 18, minibatch 900. Loss: 0.3056.\n",
      "Epoch 18 finished! It took: 4.4861 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.45 %\n",
      "At epoch 19, minibatch 0. Loss: 0.3714.\n",
      "At epoch 19, minibatch 300. Loss: 0.1962.\n",
      "At epoch 19, minibatch 600. Loss: 0.3190.\n",
      "At epoch 19, minibatch 900. Loss: 0.3739.\n",
      "Epoch 19 finished! It took: 4.3686 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.85 %\n",
      "At epoch 20, minibatch 0. Loss: 0.1781.\n",
      "At epoch 20, minibatch 300. Loss: 0.3139.\n",
      "At epoch 20, minibatch 600. Loss: 0.1405.\n",
      "At epoch 20, minibatch 900. Loss: 0.3803.\n",
      "Epoch 20 finished! It took: 4.5625 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.62 %\n",
      "At epoch 21, minibatch 0. Loss: 0.1479.\n",
      "At epoch 21, minibatch 300. Loss: 0.1627.\n",
      "At epoch 21, minibatch 600. Loss: 0.3245.\n",
      "At epoch 21, minibatch 900. Loss: 0.1126.\n",
      "Epoch 21 finished! It took: 4.4809 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.69 %\n",
      "At epoch 22, minibatch 0. Loss: 0.1580.\n",
      "At epoch 22, minibatch 300. Loss: 0.5756.\n",
      "At epoch 22, minibatch 600. Loss: 0.2027.\n",
      "At epoch 22, minibatch 900. Loss: 0.2355.\n",
      "Epoch 22 finished! It took: 4.4977 seconds\n",
      "Accuracy of the network on the 10000 test images: 93.16 %\n",
      "At epoch 23, minibatch 0. Loss: 0.2068.\n",
      "At epoch 23, minibatch 300. Loss: 0.3364.\n",
      "At epoch 23, minibatch 600. Loss: 0.3361.\n",
      "At epoch 23, minibatch 900. Loss: 0.1303.\n",
      "Epoch 23 finished! It took: 4.4743 seconds\n",
      "Accuracy of the network on the 10000 test images: 93.19 %\n",
      "At epoch 24, minibatch 0. Loss: 0.2860.\n",
      "At epoch 24, minibatch 300. Loss: 0.1141.\n",
      "At epoch 24, minibatch 600. Loss: 0.3708.\n",
      "At epoch 24, minibatch 900. Loss: 0.1284.\n",
      "Epoch 24 finished! It took: 4.4490 seconds\n",
      "Accuracy of the network on the 10000 test images: 91.66 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    start = timer()\n",
    "    for batch_num , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "    \n",
    "        the_batch = Variable(minibatch_of_images.view(-1, 28*28))\n",
    "        labels = Variable(minibatch_of_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = the_net(the_batch)\n",
    "        \n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        # we want to check the accuracy with test dataset every 300 iterations.\n",
    "        if batch_num % 300 == 0:\n",
    "            print(\"At epoch %i, minibatch %i. Loss: %.4f.\" % (epoch, batch_num, loss.data[0]))\n",
    "            \n",
    "    end = timer()\n",
    "    print(\"Epoch %i finished! It took: %.4f seconds\" % (epoch, end - start))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = the_net(Variable(images.view(-1,28*28)))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    print('Accuracy of the network on the %d test images: %.2f %%' % (total, 100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Derek Doran, Dept. of CSE, Wright State University, for ATRC Summer 2018. \n",
    "\n",
    "Homepage: https://derk--.github.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-osgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
