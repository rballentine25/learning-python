{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: A simple two layer fully connected network\n",
    "\n",
    "Here we will go step by step through implmenting a very simple 2 layer fully connected network to label MNIST digits.\n",
    "\n",
    "We start with a number of extra imports: pyplot lets us create plots, torch.nn is a torch library for implementing neural nets, and transforms is a torchvision package to help us work with image data, and datasets is a torchvision package granting us access to some built in test data. We will look at importing data in a later example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Compose() is a torchvision function that allows multiple image transformations to be chained together (ex. flipping, resizing, etc).  \n",
    "Here, Compose() is used to chain transformations of converting to a tensor and normalizing the pixel values.   \n",
    "\n",
    "ToTensor() converts an image to a PyTorch tensor and scales pixel values from 0,255 to 0,1  \n",
    "Normalize(u, o) normalizes the tensor with equation (X-u)/o where u= mean and o= standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the pixel values to [-1,1] with mean 0.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "#MNIST is a built-in dataset included with PyTorch\n",
    "training_data = dsets.MNIST(root=\"./data\", train = True, transform=transform, download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a **validation set** of data as well, but the built in MNIST caller does not support a validation set, and we will just use the testing data set as the validation set for purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = dsets.MNIST(root=\"./data\", train = False, transform=transform, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfe53a08d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The mnist data comes in as a tuple. The first index is the pixel values packed in a pytorch Tensor, \n",
    "## the 2nd index is the class label.\n",
    "# To show the image I have to make pytorch tensor a \n",
    "# seleccting the data (the pixel values) of the 891st image in the training dataset\n",
    "training_image_tensor = training_data[890][0]\n",
    "\n",
    "# Note that the image is a 1x28x28 tensor: there is only one color channel; it is grayscale. \n",
    "print(training_image_tensor.shape)\n",
    "plt.imshow(training_image_tensor[0])\n",
    "\n",
    "# note: if kernel keeps dying at this cell, check numpy version compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "class_label = training_data[890][1]\n",
    "print(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch comes with **built-in DataLoaders** that make it trivial to create minibatches and to enumerate over them. Always use these helpers!!\n",
    "\n",
    "A DataLoader takes as input the desired minibatch size you would like (batch_size), and whether or not you would like the data to be shuffled before it is partitioned into minibatches. No shuffling means the exact same minibatches will be presented to your network every epoch. There is not a lot of guidance as to whether shuffling is good or bad, but intuition says shuffling should help with generalizability a bit.\n",
    "\n",
    "The DataLoader function returns an *iterator* over your datasets with shuffling and batchsize specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data, batch_size=len(testing_data), shuffle=False)\n",
    "\n",
    "len(train_loader)\n",
    "\n",
    "# note: the batch size is the number of batches, not the number of samples per batch. 1/64th of the dataset\n",
    "# is the length returned by train_loader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a deep net\n",
    "### 1. Specify and instantiate the network.\n",
    "\n",
    "Here we go. A DNN is specified as a class that inherets from nn.Module. They all have the same structure:  \n",
    "1. a call to the constructor of nn.Module (super)\n",
    "2. specification of a number of network layers. It is common practice to wrap the way the nodes are wired and the activation function (and any other processing, like doing batch norm) inside of an nn.Sequential object. nn.Sequential is just a convinent wrapper of a number of nn.function calls. \n",
    "3. Definition of a **forward** function. This function must be specified. This function describes the way that data will pass through your deep net. Here, we see that the input will bass through fc1, then fc2, and we return thr output of fc2.   \n",
    "        \n",
    "A few things to note:   \n",
    "1. We do not need to tell the deep net what the size of the minibatch we will present to it is.\n",
    "2. nn.Linear specifies a linear combination of inputs into a layer. The linear combination is the $ \\sum_{i=1}^{n} w_i x_i$ we are used to seeing, where the summation is over **every node in the previous layer**. So nn.Linear specifies a fully-connected connectivity in this layer. \n",
    "3. We simply output the ReLU activation values of fc2 -- we do not normalize these values so they sum to 1. We may want to do a normalization here if we want the network to output a score for the likelihood the input is of each class that looks like a probability. We could do so by specifiying a **softmax** layer. But this is equivalent to just predicting the class corresponding to the fc2 output that is maximum.  \n",
    "    \n",
    "\n",
    "**The PyTorch docs are a great place to see what types of layers and activations are implemented**. Of course, nothing is stoping you from implementing your own layer or activation function and then call those functions inside of the deep net specification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The nn.Module class establishes automatic gradient calculation, layer initialization, and module registration, among other features. It is the base class for all nueral network modules in PyTorch.  \n",
    "\n",
    "The nn.Sequential class is the container module that allows NN layers to be stacked in sequential order. PyTorch automatically calculates the forward pass when Sequential is used.   \n",
    "\n",
    "nn.Linear defines a layer with a linear transformation: y = wx + b, where w is weight matrix, x is input vector, y is output vector, and b is bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):                                       # defining class, inheriting from nn.Module\n",
    "    def __init__(self, input_size, hidden_size, num_classes):   # init is constructor\n",
    "        # init takes sizes for the input layer, hidden layers, and the number of classes\n",
    "        super(DeepNet, self).__init__()                         # Call to constructor of nn.Module\n",
    "        \n",
    "        # adding two fully connected (fc) layers in the network\n",
    "        self.fc1 = nn.Sequential(                               #  Sequential allows layers to be stacked\n",
    "            nn.Linear(input_size, hidden_size),                 # establishes linear transformation for layer\n",
    "            nn.ReLU()                                           # ReLu activation function after linear trans\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # output layer is third fc layer in the network (looks just like previous two)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "\n",
    "    # forward method defines the forward pass computation for the network, with input x being passed to method\n",
    "    # forward is a required method for an inheritor of nn.Module and is called by default with netname(input) syntax\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)                                       # x is passed through first fc layer\n",
    "        out = self.fc2(out)                                     # output of 1st layer is used as input for 2nd\n",
    "        out = self.output(out)                                  # output is passed through last layer as input\n",
    "        return out                                              # final output is returned (output of final layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0061,  0.0192, -0.0299,  ..., -0.0052, -0.0182, -0.0077],\n",
      "        [ 0.0270, -0.0143,  0.0141,  ..., -0.0180, -0.0074,  0.0136],\n",
      "        [-0.0221,  0.0072,  0.0047,  ...,  0.0164,  0.0240,  0.0164],\n",
      "        ...,\n",
      "        [-0.0080,  0.0092,  0.0285,  ..., -0.0111, -0.0175,  0.0297],\n",
      "        [-0.0137,  0.0063,  0.0134,  ...,  0.0120, -0.0013, -0.0006],\n",
      "        [ 0.0131, -0.0075,  0.0312,  ..., -0.0018,  0.0285,  0.0034]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0145,  0.0045, -0.0029,  0.0089, -0.0229,  0.0136, -0.0340, -0.0329,\n",
      "         0.0308, -0.0201], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.8779e-01, -2.0579e-02, -1.1819e-01, -5.1978e-02,  1.5964e-01,\n",
      "          2.8566e-01,  1.8838e-01, -1.8009e-01, -1.2404e-01,  3.0755e-01],\n",
      "        [-1.0570e-01,  2.9516e-01,  1.9236e-01,  2.3086e-01, -1.8839e-01,\n",
      "         -8.7887e-03, -1.4656e-01, -1.2832e-01,  1.9064e-01,  2.5751e-01],\n",
      "        [-1.9992e-01, -2.4870e-01,  6.1853e-02,  2.3742e-01, -2.7928e-01,\n",
      "         -1.1807e-01, -9.0737e-02,  9.5254e-02, -8.2607e-02, -2.7947e-01],\n",
      "        [ 1.7200e-01,  1.8280e-01,  1.3564e-01,  5.7892e-02,  1.7359e-01,\n",
      "          1.8729e-01,  1.2380e-01,  2.2172e-01, -1.9761e-01, -8.9212e-03],\n",
      "        [ 2.2474e-01,  3.0406e-01, -3.1079e-01,  2.8192e-01, -8.2200e-02,\n",
      "         -1.9155e-01, -3.1580e-01,  2.8922e-01,  2.5346e-01, -2.3396e-01],\n",
      "        [ 5.1279e-02,  1.9257e-01,  2.8151e-04,  4.1773e-02,  3.0210e-01,\n",
      "          1.3665e-01, -1.2017e-01, -1.0896e-01,  1.8453e-01,  1.1420e-01],\n",
      "        [-2.3887e-01,  2.4975e-02, -1.1959e-01, -2.1631e-03, -1.1719e-01,\n",
      "         -2.8425e-01, -7.1760e-02, -1.1568e-01,  2.1257e-01,  7.8959e-02],\n",
      "        [ 1.2380e-01, -1.1872e-01,  6.2945e-02, -3.0088e-01,  2.0120e-01,\n",
      "         -4.7041e-02, -3.0683e-02, -1.7316e-01,  9.1677e-02, -2.8787e-01],\n",
      "        [-1.1833e-01, -2.9829e-01, -2.9889e-01, -3.0018e-01,  1.7056e-01,\n",
      "         -2.1013e-01, -1.2670e-01, -4.2723e-02,  2.1730e-01, -3.1989e-02],\n",
      "        [ 6.1977e-02,  7.5111e-02, -1.6613e-02,  2.1787e-01,  1.7851e-01,\n",
      "          1.6292e-01,  1.4926e-01,  1.1873e-01,  2.6620e-01, -3.7791e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2100,  0.2317,  0.2471,  0.0147, -0.2752,  0.0699, -0.0229, -0.1599,\n",
      "        -0.1683,  0.2025], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1922,  0.1030,  0.1841,  0.0134, -0.2148, -0.2147, -0.2297,  0.0023,\n",
      "          0.3127, -0.0216],\n",
      "        [ 0.2459,  0.0755, -0.1462, -0.1465, -0.1030, -0.2217, -0.0669, -0.2287,\n",
      "          0.0091,  0.2808],\n",
      "        [ 0.2642,  0.2634, -0.0859,  0.2321,  0.0642,  0.0187, -0.0289,  0.1796,\n",
      "         -0.1148, -0.0326],\n",
      "        [ 0.1762, -0.0389, -0.2753,  0.2031, -0.1369,  0.0866,  0.2518,  0.1626,\n",
      "          0.2425, -0.1978],\n",
      "        [-0.2629,  0.1402, -0.2328, -0.0066,  0.1973, -0.0162,  0.1314,  0.1567,\n",
      "         -0.0435, -0.0637],\n",
      "        [ 0.3053,  0.2693, -0.1317,  0.2350, -0.1874,  0.0954,  0.0007,  0.2324,\n",
      "         -0.1878, -0.0980],\n",
      "        [-0.0375,  0.1894, -0.1299, -0.2182, -0.2589,  0.0830, -0.1501, -0.0366,\n",
      "         -0.0097,  0.1595],\n",
      "        [ 0.1553,  0.0393, -0.1829, -0.2444, -0.1136,  0.2405,  0.2605, -0.2211,\n",
      "         -0.0660, -0.1572],\n",
      "        [-0.0937,  0.1508,  0.2964, -0.0450, -0.2837,  0.0750,  0.1030,  0.0652,\n",
      "          0.1636, -0.2979],\n",
      "        [ 0.0853, -0.0035,  0.1958, -0.1280, -0.1463,  0.1102, -0.1146, -0.0114,\n",
      "          0.0366, -0.0732]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2035, -0.1317, -0.2467, -0.2484,  0.0143, -0.1478, -0.0695,  0.1310,\n",
      "         0.1884,  0.0968], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0061,  0.0192, -0.0299,  ..., -0.0052, -0.0182, -0.0077],\n",
      "        [ 0.0270, -0.0143,  0.0141,  ..., -0.0180, -0.0074,  0.0136],\n",
      "        [-0.0221,  0.0072,  0.0047,  ...,  0.0164,  0.0240,  0.0164],\n",
      "        ...,\n",
      "        [-0.0080,  0.0092,  0.0285,  ..., -0.0111, -0.0175,  0.0297],\n",
      "        [-0.0137,  0.0063,  0.0134,  ...,  0.0120, -0.0013, -0.0006],\n",
      "        [ 0.0131, -0.0075,  0.0312,  ..., -0.0018,  0.0285,  0.0034]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0145,  0.0045, -0.0029,  0.0089, -0.0229,  0.0136, -0.0340, -0.0329,\n",
      "         0.0308, -0.0201], requires_grad=True), Parameter containing:\n",
      "tensor([[ 9.9964, 10.0090,  9.9887, 10.0041,  9.9857,  9.9965, 10.0257,  9.9953,\n",
      "          9.9989,  9.9994],\n",
      "        [10.0195,  9.9903, 10.0015, 10.0090,  9.9948,  9.9868,  9.9981,  9.9975,\n",
      "         10.0028,  9.9835],\n",
      "        [10.0070, 10.0048, 10.0068, 10.0026, 10.0102, 10.0066,  9.9897,  9.9996,\n",
      "          9.9998,  9.9985],\n",
      "        [ 9.9990,  9.9929, 10.0062,  9.9963, 10.0005, 10.0026, 10.0136,  9.9923,\n",
      "          9.9833,  9.9936],\n",
      "        [ 9.9998, 10.0062,  9.9945, 10.0047, 10.0050, 10.0067, 10.0155,  9.9819,\n",
      "         10.0062,  9.9874],\n",
      "        [ 9.9862,  9.9912,  9.9938, 10.0062, 10.0055,  9.9847,  9.9921,  9.9990,\n",
      "          9.9781,  9.9932],\n",
      "        [ 9.9941, 10.0048,  9.9928, 10.0013,  9.9931,  9.9969,  9.9885,  9.9977,\n",
      "          9.9938,  9.9961],\n",
      "        [ 9.9981, 10.0007,  9.9848,  9.9985,  9.9949, 10.0013,  9.9989, 10.0035,\n",
      "          9.9968, 10.0274],\n",
      "        [10.0053,  9.9930,  9.9995,  9.9960, 10.0221, 10.0080,  9.9962,  9.9965,\n",
      "          9.9781,  9.9930],\n",
      "        [10.0041, 10.0079, 10.0025, 10.0104,  9.9971, 10.0080,  9.9977,  9.9906,\n",
      "          9.9985, 10.0015]], requires_grad=True), Parameter containing:\n",
      "tensor([49.9808, 50.0031, 50.0024, 49.9981, 49.9979, 50.0000, 50.0101, 50.0056,\n",
      "        50.0016, 49.9965], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1922,  0.1030,  0.1841,  0.0134, -0.2148, -0.2147, -0.2297,  0.0023,\n",
      "          0.3127, -0.0216],\n",
      "        [ 0.2459,  0.0755, -0.1462, -0.1465, -0.1030, -0.2217, -0.0669, -0.2287,\n",
      "          0.0091,  0.2808],\n",
      "        [ 0.2642,  0.2634, -0.0859,  0.2321,  0.0642,  0.0187, -0.0289,  0.1796,\n",
      "         -0.1148, -0.0326],\n",
      "        [ 0.1762, -0.0389, -0.2753,  0.2031, -0.1369,  0.0866,  0.2518,  0.1626,\n",
      "          0.2425, -0.1978],\n",
      "        [-0.2629,  0.1402, -0.2328, -0.0066,  0.1973, -0.0162,  0.1314,  0.1567,\n",
      "         -0.0435, -0.0637],\n",
      "        [ 0.3053,  0.2693, -0.1317,  0.2350, -0.1874,  0.0954,  0.0007,  0.2324,\n",
      "         -0.1878, -0.0980],\n",
      "        [-0.0375,  0.1894, -0.1299, -0.2182, -0.2589,  0.0830, -0.1501, -0.0366,\n",
      "         -0.0097,  0.1595],\n",
      "        [ 0.1553,  0.0393, -0.1829, -0.2444, -0.1136,  0.2405,  0.2605, -0.2211,\n",
      "         -0.0660, -0.1572],\n",
      "        [-0.0937,  0.1508,  0.2964, -0.0450, -0.2837,  0.0750,  0.1030,  0.0652,\n",
      "          0.1636, -0.2979],\n",
      "        [ 0.0853, -0.0035,  0.1958, -0.1280, -0.1463,  0.1102, -0.1146, -0.0114,\n",
      "          0.0366, -0.0732]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2035, -0.1317, -0.2467, -0.2484,  0.0143, -0.1478, -0.0695,  0.1310,\n",
      "         0.1884,  0.0968], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0084,  0.0345,  0.0300,  ..., -0.0115,  0.0012, -0.0054],\n",
      "        [ 0.0015,  0.0088, -0.0025,  ...,  0.0309, -0.0180,  0.0278],\n",
      "        [ 0.0142,  0.0299,  0.0090,  ..., -0.0287, -0.0208, -0.0032],\n",
      "        ...,\n",
      "        [-0.0022, -0.0340,  0.0132,  ...,  0.0122, -0.0282, -0.0117],\n",
      "        [-0.0197,  0.0227,  0.0209,  ..., -0.0350,  0.0342, -0.0251],\n",
      "        [ 0.0180,  0.0098,  0.0099,  ..., -0.0115, -0.0088, -0.0009]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0093,  0.0126,  0.0260, -0.0170,  0.0164, -0.0332, -0.0338, -0.0130,\n",
      "         0.0155,  0.0104], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2790, -0.0939, -0.1605, -0.1003,  0.2988, -0.0588,  0.3109,  0.2917,\n",
      "          0.2896, -0.1939],\n",
      "        [-0.1890, -0.2081,  0.0465,  0.2748, -0.2558,  0.0504, -0.1112, -0.3044,\n",
      "          0.2182,  0.2902],\n",
      "        [ 0.0760,  0.0990, -0.2220,  0.1847, -0.0868, -0.1477, -0.2183, -0.2055,\n",
      "         -0.2841, -0.1776],\n",
      "        [ 0.0539,  0.0822, -0.0393,  0.0152,  0.2674,  0.2368,  0.2031, -0.1478,\n",
      "         -0.0160, -0.1186],\n",
      "        [ 0.1894, -0.1260,  0.1558, -0.0025, -0.1842,  0.1112, -0.2287,  0.2698,\n",
      "         -0.0155, -0.0238],\n",
      "        [ 0.0741,  0.0114,  0.0869, -0.0405,  0.2222, -0.2693,  0.0182, -0.2251,\n",
      "         -0.2463,  0.1023],\n",
      "        [-0.1875,  0.1522,  0.2562,  0.1597, -0.1722, -0.3038,  0.1988, -0.2579,\n",
      "         -0.2765,  0.0402],\n",
      "        [-0.1599,  0.2859, -0.0905, -0.2387,  0.0899,  0.0266,  0.1341,  0.0228,\n",
      "         -0.2242, -0.1083],\n",
      "        [-0.1911,  0.0096, -0.0319,  0.1053,  0.1045,  0.0790,  0.2001,  0.1646,\n",
      "         -0.0893, -0.2984],\n",
      "        [ 0.0510,  0.2142, -0.0331, -0.2677,  0.2189,  0.2113, -0.2870,  0.1211,\n",
      "          0.1404, -0.0651]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0576,  0.2358,  0.2113,  0.0279,  0.0581, -0.0775, -0.1837,  0.2750,\n",
      "         0.2611, -0.0679], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1408,  0.0649, -0.0887, -0.0895, -0.0792, -0.0683, -0.1948,  0.3096,\n",
      "         -0.0124, -0.2322],\n",
      "        [-0.2780,  0.1726,  0.0055, -0.0211,  0.2695,  0.0860,  0.1821, -0.2236,\n",
      "         -0.1353, -0.1648],\n",
      "        [-0.0131,  0.0075, -0.2434, -0.0665,  0.0254, -0.0040,  0.0789,  0.0540,\n",
      "         -0.0708,  0.2775],\n",
      "        [ 0.2019,  0.0496, -0.1959,  0.2489,  0.2446,  0.2892, -0.1739,  0.2751,\n",
      "         -0.0542,  0.3023],\n",
      "        [ 0.0523, -0.0841, -0.1880,  0.0083, -0.1030, -0.0103,  0.1064,  0.1533,\n",
      "          0.1445,  0.2984],\n",
      "        [ 0.2353,  0.2396, -0.2033,  0.1210,  0.2491,  0.1600,  0.0800,  0.2146,\n",
      "         -0.1956, -0.0080],\n",
      "        [-0.2592, -0.0241,  0.0317, -0.1802,  0.1495, -0.2124,  0.0678, -0.1528,\n",
      "         -0.2612,  0.2575],\n",
      "        [ 0.2799, -0.1664, -0.2213,  0.0773,  0.2748, -0.2229, -0.3106,  0.2306,\n",
      "         -0.1971,  0.1699],\n",
      "        [ 0.0866, -0.1790,  0.1778,  0.1629,  0.2737,  0.2121,  0.0747,  0.2131,\n",
      "          0.0346, -0.1023],\n",
      "        [-0.2509, -0.0407,  0.2100,  0.2635, -0.0897, -0.2809,  0.1204,  0.0175,\n",
      "          0.1920, -0.2804]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0672, -0.0080,  0.2849, -0.2936, -0.0981,  0.0106, -0.2897,  0.2221,\n",
      "        -0.1810, -0.2820], requires_grad=True)]\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "number of network parameters: 8070\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTING THE DEEP NET:\n",
    "\n",
    "input_size = 28*28 ## Note that an MNIST image is 28*28 pixels.\n",
    "num_classes = 10\n",
    "hidden_size = 10    # number of hidden layers\n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(the_net)\n",
    "\n",
    "## the parameters() function of an nn.Module object returns an iterator over \n",
    "## all of the network parameters (e.g. weights.) PyTorch initializes all weights in linear and\n",
    "## conv layers as: \n",
    "## stdv = 1. / math.sqrt(number_of_layer_inputs)\n",
    "## self.weight.data.uniform_(-stdv, stdv)\n",
    "## So with uniformly distributed weights within -stdv, stdv.\n",
    "\n",
    "## If you want to see what the parameters are, iterate over parameters() or make it a list:\n",
    "params = list(the_net.parameters())\n",
    "## We print the parameters of the network in order of weights, then biases, for each layer that has weights and \n",
    "## biases, in sequential order of operation in the forward pass. \n",
    "print(params)\n",
    "\n",
    "## If you want to specify your own weight initiaization routine, you can do it simply!\n",
    "## It is possible to . through the structure of the_net. any Sequential layers are lists, so fc2[0] is the \n",
    "## nn.Linear in fc2. fc2[1] is the ReLU activation.\n",
    "## Remember that _ suffix means in place updating.\n",
    "the_net.fc2[0].weight.data.normal_(10,0.01)         # initializing weights of 2nd layer with norm distro, mean=10 std=0.01\n",
    "the_net.fc2[0].bias.data.normal_(50,0.01)           # bias of 2nd layer as norm distro, mean=50 std=0.01\n",
    "print(params)\n",
    "\n",
    "## anyway, these weights are horrible. Let's reinitialize the network. :) \n",
    "# (i.e. what we just did with the weights is overwritten back to defaults)\n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(list(the_net.parameters()))\n",
    "\n",
    "##BTW, how many parameters do we have? \n",
    "# parameters are the number of weights and biases that the network learns/adjusts during training\n",
    "# there is a bias for each node and a weight for each connection\n",
    "num_parameters = 0\n",
    "for x in the_net.parameters():\n",
    "    print(x.shape)\n",
    "for x in the_net.parameters():\n",
    "    num_params = 1\n",
    "    for dim_size in x.shape:\n",
    "        num_params *= dim_size\n",
    "    num_parameters += num_params\n",
    "print('number of network parameters: %i' % (num_parameters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define your loss function.\n",
    "\n",
    "Since we are tackling a classification problem we want to use cross entropy loss. For those who know cross entropy (or have heard of **entropy** before) would know that the loss must operate on a probability distribution. And indeed it does: this loss expects the probability the network thinks the input is of each class. In PyTorch, **this normalization (e.g. passing the output of fc2 through a *softmax layer*) is done automatically by the loss function**. I (Derek) don't really like this, its a bit of a gotcha! Now you know!  \n",
    "\n",
    "----  \n",
    "\n",
    "Cross-entropy is a measure of how different two probability distributions are. It is used to compare predicted probabilities of each class to their actual labels. The goal of training the model is to minimize the difference between the predictions and the true class labels, which is represented by the cross-entropy calculation.  \n",
    "\n",
    "Softmax is a function that turns raw model outputs into probabilities and makes sure all all converted predicted values for all classes add up to 1.  \n",
    "\n",
    "PyTorch's built in cross-entropy loss function automatically applies Softmax to the output before calculating lsos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define your optimizer.\n",
    "An optimizer is responsible for updating the model's weights and biases in order to minimize the loss function. Its goal is to find the optimal set of parameters that minimizes loss; this is done by considering the gradient of the loss function wrt the model parameters.  \n",
    "\n",
    "The most common optimization method is Gradient Decent (GD), which calculates the gradient of the loss function wrt each parameter then adjusts the parameters in the opposite direction as the gradient.\n",
    "  \n",
    "SGD is Stoichastic Gradient Descent is the simplest form of GD. The gradient is computed for a randomly selected batch of data points. This is faster but noisier than other methods.\n",
    "\n",
    "The learning rate is a hyperparameter that controls the size of the steps the optimizer takes when updating weights. The smaller the learning rate, the slower the model learns. If it is too large, the optimizer may overshoot and fail to converge.  \n",
    "\n",
    "When in doubt, start with SGD with a small learning rate parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(the_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training.\n",
    "We will train in a loop that iterates over the minibatches yielded from train_loader, for a fixed number of epochs. Here is what's going on:   \n",
    "\n",
    "1.  We are specifying a loop for 3 epochs. In the loop we wrap the train_loader in an enumerator. The enumerator will yield an enumeration count and a tuple (minibatch_of_images, minibatch_of_labels). \n",
    "2.  The minibatch_of_images is a 4D tensor with dimensions (size_of_minibatch, depth, width, height). The network expects as input a vector of size 28x28 so we need to reshape the tensor accordingly. We will reshape it to a matrix of 32 rows, 28x28 columns. Note that we can pass -1 as the first parameter of view, this is like saying \"I don't know how many rows I'll need, just make as many as needed to the column count I specify (28*28) is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs): # outer loop runs 3 times for number of epochs\n",
    "    # _ is placeholder for variable name\n",
    "    for _ , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "        print(minibatch_of_images.shape)\n",
    "        print(minibatch_of_images.view(batch_size, 28*28).shape)        # 32 images, 28x28 each\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> We build a computation graph here to setup backprop! So wrap the labels and the batch in a Variable.\n",
    "    <li> Tell the optimizer to zero out any gradient stored in the Variables of the parameters of the_net. Indeed, the optimizer knows of the_net's parameters as we passed them in as a required parameter of the optimizer above. **Zero the gradient out after every minibatch!**\n",
    "     <li> Okay, run the_batch through the_net. This is the **forward pass**. Note that outputs will be a Variable.\n",
    "     <li> Compute the loss, how much error the network had on the_batch.\n",
    "     <li> Now backprop! loss is a real number value, so that will be the gradient signal. Observe. loss is a functino of the outputs, which is a function of the_net, which is a function of the_batch. So the computation chain is all setup! \n",
    "      <li> Tell the optimizer to update the weights by calling step(). Step tells the optimizer to use the gradients stored at each network parameter (computed by calling backward() on loss) to compute and apply a weight update.\n",
    "      <li> Give some console output to see how we are doing every few minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, minibatch 0. Loss: 2.2984.\n",
      "At epoch 0, minibatch 300. Loss: 2.2593.\n",
      "At epoch 0, minibatch 600. Loss: 2.0734.\n",
      "At epoch 0, minibatch 900. Loss: 1.8000.\n",
      "Epoch 0 finished! It took: 6.8615 seconds\n",
      "Accuracy of the network on the 10000 test images: 41.26 %\n",
      "At epoch 1, minibatch 0. Loss: 1.8124.\n",
      "At epoch 1, minibatch 300. Loss: 1.3816.\n",
      "At epoch 1, minibatch 600. Loss: 1.6489.\n",
      "At epoch 1, minibatch 900. Loss: 1.5879.\n",
      "Epoch 1 finished! It took: 6.9093 seconds\n",
      "Accuracy of the network on the 10000 test images: 54.13 %\n",
      "At epoch 2, minibatch 0. Loss: 1.3829.\n",
      "At epoch 2, minibatch 300. Loss: 1.3516.\n",
      "At epoch 2, minibatch 600. Loss: 1.2018.\n",
      "At epoch 2, minibatch 900. Loss: 1.2414.\n",
      "Epoch 2 finished! It took: 6.9418 seconds\n",
      "Accuracy of the network on the 10000 test images: 55.14 %\n",
      "At epoch 3, minibatch 0. Loss: 0.9728.\n",
      "At epoch 3, minibatch 300. Loss: 1.2319.\n",
      "At epoch 3, minibatch 600. Loss: 1.1052.\n",
      "At epoch 3, minibatch 900. Loss: 1.2578.\n",
      "Epoch 3 finished! It took: 7.0567 seconds\n",
      "Accuracy of the network on the 10000 test images: 55.67 %\n",
      "At epoch 4, minibatch 0. Loss: 1.0957.\n",
      "At epoch 4, minibatch 300. Loss: 0.9849.\n",
      "At epoch 4, minibatch 600. Loss: 1.1340.\n",
      "At epoch 4, minibatch 900. Loss: 1.2681.\n",
      "Epoch 4 finished! It took: 7.0241 seconds\n",
      "Accuracy of the network on the 10000 test images: 55.89 %\n",
      "At epoch 5, minibatch 0. Loss: 1.1826.\n",
      "At epoch 5, minibatch 300. Loss: 1.1047.\n",
      "At epoch 5, minibatch 600. Loss: 1.2117.\n",
      "At epoch 5, minibatch 900. Loss: 0.9038.\n",
      "Epoch 5 finished! It took: 6.8966 seconds\n",
      "Accuracy of the network on the 10000 test images: 56.27 %\n",
      "At epoch 6, minibatch 0. Loss: 0.9366.\n",
      "At epoch 6, minibatch 300. Loss: 1.1065.\n",
      "At epoch 6, minibatch 600. Loss: 1.0475.\n",
      "At epoch 6, minibatch 900. Loss: 1.1486.\n",
      "Epoch 6 finished! It took: 6.9635 seconds\n",
      "Accuracy of the network on the 10000 test images: 56.47 %\n",
      "At epoch 7, minibatch 0. Loss: 1.0432.\n",
      "At epoch 7, minibatch 300. Loss: 1.2884.\n",
      "At epoch 7, minibatch 600. Loss: 1.1949.\n",
      "At epoch 7, minibatch 900. Loss: 1.0725.\n",
      "Epoch 7 finished! It took: 6.8358 seconds\n",
      "Accuracy of the network on the 10000 test images: 56.65 %\n",
      "At epoch 8, minibatch 0. Loss: 1.2582.\n",
      "At epoch 8, minibatch 300. Loss: 1.3581.\n",
      "At epoch 8, minibatch 600. Loss: 1.2994.\n",
      "At epoch 8, minibatch 900. Loss: 1.1699.\n",
      "Epoch 8 finished! It took: 6.8021 seconds\n",
      "Accuracy of the network on the 10000 test images: 57.34 %\n",
      "At epoch 9, minibatch 0. Loss: 1.1516.\n",
      "At epoch 9, minibatch 300. Loss: 1.0408.\n",
      "At epoch 9, minibatch 600. Loss: 0.9464.\n",
      "At epoch 9, minibatch 900. Loss: 0.7537.\n",
      "Epoch 9 finished! It took: 6.8373 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.13 %\n",
      "At epoch 10, minibatch 0. Loss: 0.8439.\n",
      "At epoch 10, minibatch 300. Loss: 1.0877.\n",
      "At epoch 10, minibatch 600. Loss: 0.6712.\n",
      "At epoch 10, minibatch 900. Loss: 0.8605.\n",
      "Epoch 10 finished! It took: 6.9032 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.30 %\n",
      "At epoch 11, minibatch 0. Loss: 1.1511.\n",
      "At epoch 11, minibatch 300. Loss: 0.9007.\n",
      "At epoch 11, minibatch 600. Loss: 0.8244.\n",
      "At epoch 11, minibatch 900. Loss: 0.6829.\n",
      "Epoch 11 finished! It took: 6.8214 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.38 %\n",
      "At epoch 12, minibatch 0. Loss: 1.1561.\n",
      "At epoch 12, minibatch 300. Loss: 1.0274.\n",
      "At epoch 12, minibatch 600. Loss: 0.7849.\n",
      "At epoch 12, minibatch 900. Loss: 0.7415.\n",
      "Epoch 12 finished! It took: 6.7941 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.62 %\n",
      "At epoch 13, minibatch 0. Loss: 0.7977.\n",
      "At epoch 13, minibatch 300. Loss: 0.9639.\n",
      "At epoch 13, minibatch 600. Loss: 0.4786.\n",
      "At epoch 13, minibatch 900. Loss: 0.8234.\n",
      "Epoch 13 finished! It took: 6.9454 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.81 %\n",
      "At epoch 14, minibatch 0. Loss: 0.6530.\n",
      "At epoch 14, minibatch 300. Loss: 0.7545.\n",
      "At epoch 14, minibatch 600. Loss: 0.8735.\n",
      "At epoch 14, minibatch 900. Loss: 0.6379.\n",
      "Epoch 14 finished! It took: 7.2076 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.86 %\n",
      "At epoch 15, minibatch 0. Loss: 0.6718.\n",
      "At epoch 15, minibatch 300. Loss: 0.6841.\n",
      "At epoch 15, minibatch 600. Loss: 0.8265.\n",
      "At epoch 15, minibatch 900. Loss: 0.4882.\n",
      "Epoch 15 finished! It took: 7.0927 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.12 %\n",
      "At epoch 16, minibatch 0. Loss: 0.8913.\n",
      "At epoch 16, minibatch 300. Loss: 0.5935.\n",
      "At epoch 16, minibatch 600. Loss: 0.8040.\n",
      "At epoch 16, minibatch 900. Loss: 0.7290.\n",
      "Epoch 16 finished! It took: 6.9168 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.29 %\n",
      "At epoch 17, minibatch 0. Loss: 0.5563.\n",
      "At epoch 17, minibatch 300. Loss: 0.6894.\n",
      "At epoch 17, minibatch 600. Loss: 0.6735.\n",
      "At epoch 17, minibatch 900. Loss: 0.8433.\n",
      "Epoch 17 finished! It took: 6.7882 seconds\n",
      "Accuracy of the network on the 10000 test images: 73.99 %\n",
      "At epoch 18, minibatch 0. Loss: 0.6892.\n",
      "At epoch 18, minibatch 300. Loss: 0.7432.\n",
      "At epoch 18, minibatch 600. Loss: 0.6573.\n",
      "At epoch 18, minibatch 900. Loss: 1.1263.\n",
      "Epoch 18 finished! It took: 6.8261 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.28 %\n",
      "At epoch 19, minibatch 0. Loss: 0.7806.\n",
      "At epoch 19, minibatch 300. Loss: 0.7647.\n",
      "At epoch 19, minibatch 600. Loss: 0.6162.\n",
      "At epoch 19, minibatch 900. Loss: 0.6806.\n",
      "Epoch 19 finished! It took: 6.8520 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.34 %\n",
      "At epoch 20, minibatch 0. Loss: 0.8348.\n",
      "At epoch 20, minibatch 300. Loss: 0.9855.\n",
      "At epoch 20, minibatch 600. Loss: 0.5824.\n",
      "At epoch 20, minibatch 900. Loss: 0.7134.\n",
      "Epoch 20 finished! It took: 6.8888 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.46 %\n",
      "At epoch 21, minibatch 0. Loss: 1.1502.\n",
      "At epoch 21, minibatch 300. Loss: 0.6276.\n",
      "At epoch 21, minibatch 600. Loss: 0.7650.\n",
      "At epoch 21, minibatch 900. Loss: 0.6891.\n",
      "Epoch 21 finished! It took: 6.9049 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.87 %\n",
      "At epoch 22, minibatch 0. Loss: 0.5857.\n",
      "At epoch 22, minibatch 300. Loss: 0.8265.\n",
      "At epoch 22, minibatch 600. Loss: 0.5769.\n",
      "At epoch 22, minibatch 900. Loss: 0.9493.\n",
      "Epoch 22 finished! It took: 7.0772 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.48 %\n",
      "At epoch 23, minibatch 0. Loss: 0.7083.\n",
      "At epoch 23, minibatch 300. Loss: 0.6442.\n",
      "At epoch 23, minibatch 600. Loss: 0.6135.\n",
      "At epoch 23, minibatch 900. Loss: 0.8250.\n",
      "Epoch 23 finished! It took: 7.2029 seconds\n",
      "Accuracy of the network on the 10000 test images: 74.62 %\n",
      "At epoch 24, minibatch 0. Loss: 0.7737.\n",
      "At epoch 24, minibatch 300. Loss: 0.8185.\n",
      "At epoch 24, minibatch 600. Loss: 0.6571.\n",
      "At epoch 24, minibatch 900. Loss: 0.6801.\n",
      "Epoch 24 finished! It took: 6.9947 seconds\n",
      "Accuracy of the network on the 10000 test images: 75.06 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    start = timer()\n",
    "    for batch_num , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "    \n",
    "        # wrap batch and labels in Variables\n",
    "        the_batch = Variable(minibatch_of_images.view(-1, 28*28))  \n",
    "        labels = Variable(minibatch_of_labels)\n",
    "        \n",
    "        # zero out any stored gradients in the Variables (should be done every batch)\n",
    "        optimizer.zero_grad()                   \n",
    "        \n",
    "        # run forward pass: auto calls forward() method with given input\n",
    "        output = the_net(the_batch)             \n",
    "        \n",
    "        # calculate loss on the output of the forward pass\n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        # pytorch autograd has built-in back propogation function. The function traverses the computational graph\n",
    "        # automatically built during the forward pass to compute the gradient of the loss wrt each parameter\n",
    "        loss.backward()\n",
    "        \n",
    "        # instruct the optimizer to adjust weights/biases by one step (0.01) considering results of loss function\n",
    "        optimizer.step()\n",
    "        \n",
    "        # max returns the max and the location of the max of the output data, along dimension 1.\n",
    "        # _ means that actual max is disregarded, but index of max score is corresponds to the predicted label.\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        # we want to check the accuracy with test dataset every 300 iterations.\n",
    "        if batch_num % 300 == 0:\n",
    "            print(\"At epoch %i, minibatch %i. Loss: %.4f.\" % (epoch, batch_num, loss.item()))\n",
    "            \n",
    "    end = timer()\n",
    "    print(\"Epoch %i finished! It took: %.4f seconds\" % (epoch, end - start))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = the_net(Variable(images.view(-1,28*28)))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    print('Accuracy of the network on the %d test images: %.2f %%' % (total, 100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 25 epochs with LR=0.01, accuracy was 75.06%. Time to run was 3m15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Derek Doran, Dept. of CSE, Wright State University, for ATRC Summer 2018. \n",
    "\n",
    "Homepage: https://derk--.github.io/\n",
    "\n",
    "*edited 2-3-2025 by Rachael Ballentine*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
